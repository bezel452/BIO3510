{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取和预处理\n",
    "# mut_matrix是提供的突变矩阵，data_y是样本的疾病状态标签\n",
    "data = pd.read_csv(\"./clean_data_final.csv\")\n",
    "data_y = data[\"overall_survival\"]\n",
    "mut_matrix = data.iloc[:,497:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集和批量大小\n",
    "data = TensorDataset(torch.from_numpy(mut_matrix.values).float(), torch.from_numpy(data_y.values).float())\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练集和验证集的比例\n",
    "train_ratio = 0.8\n",
    "valid_ratio = 0.2\n",
    "\n",
    "# 计算对应的数据量\n",
    "train_size = int(train_ratio * len(data))\n",
    "valid_size = len(data) - train_size\n",
    "\n",
    "# 使用 random_split() 随机分配训练集和验证集\n",
    "train_set, valid_set = random_split(data, [train_size, valid_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = valid_set.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/24], Loss: 55.7305\n",
      "Epoch [1/50], Step [20/24], Loss: 22.9559\n",
      "Epoch [2/50], Step [10/24], Loss: 19.3815\n",
      "Epoch [2/50], Step [20/24], Loss: 20.8169\n",
      "Epoch [3/50], Step [10/24], Loss: 19.1017\n",
      "Epoch [3/50], Step [20/24], Loss: 19.8626\n",
      "Epoch [4/50], Step [10/24], Loss: 18.8397\n",
      "Epoch [4/50], Step [20/24], Loss: 19.5237\n",
      "Epoch [5/50], Step [10/24], Loss: 18.4583\n",
      "Epoch [5/50], Step [20/24], Loss: 19.3582\n",
      "Epoch [6/50], Step [10/24], Loss: 18.4667\n",
      "Epoch [6/50], Step [20/24], Loss: 19.3943\n",
      "Epoch [7/50], Step [10/24], Loss: 18.5105\n",
      "Epoch [7/50], Step [20/24], Loss: 19.3567\n",
      "Epoch [8/50], Step [10/24], Loss: 18.5992\n",
      "Epoch [8/50], Step [20/24], Loss: 19.4463\n",
      "Epoch [9/50], Step [10/24], Loss: 18.6196\n",
      "Epoch [9/50], Step [20/24], Loss: 19.3839\n",
      "Epoch [10/50], Step [10/24], Loss: 18.5940\n",
      "Epoch [10/50], Step [20/24], Loss: 19.3321\n",
      "Epoch [11/50], Step [10/24], Loss: 18.5320\n",
      "Epoch [11/50], Step [20/24], Loss: 19.5412\n",
      "Epoch [12/50], Step [10/24], Loss: 18.4602\n",
      "Epoch [12/50], Step [20/24], Loss: 19.3595\n",
      "Epoch [13/50], Step [10/24], Loss: 18.4740\n",
      "Epoch [13/50], Step [20/24], Loss: 19.5160\n",
      "Epoch [14/50], Step [10/24], Loss: 18.5307\n",
      "Epoch [14/50], Step [20/24], Loss: 19.3493\n",
      "Epoch [15/50], Step [10/24], Loss: 18.3913\n",
      "Epoch [15/50], Step [20/24], Loss: 19.4759\n",
      "Epoch [16/50], Step [10/24], Loss: 18.5586\n",
      "Epoch [16/50], Step [20/24], Loss: 19.5823\n",
      "Epoch [17/50], Step [10/24], Loss: 18.5116\n",
      "Epoch [17/50], Step [20/24], Loss: 19.4693\n",
      "Epoch [18/50], Step [10/24], Loss: 18.6367\n",
      "Epoch [18/50], Step [20/24], Loss: 19.5351\n",
      "Epoch [19/50], Step [10/24], Loss: 18.5591\n",
      "Epoch [19/50], Step [20/24], Loss: 19.3301\n",
      "Epoch [20/50], Step [10/24], Loss: 18.4535\n",
      "Epoch [20/50], Step [20/24], Loss: 19.4777\n",
      "Epoch [21/50], Step [10/24], Loss: 18.3632\n",
      "Epoch [21/50], Step [20/24], Loss: 19.3198\n",
      "Epoch [22/50], Step [10/24], Loss: 18.3502\n",
      "Epoch [22/50], Step [20/24], Loss: 19.3993\n",
      "Epoch [23/50], Step [10/24], Loss: 18.6430\n",
      "Epoch [23/50], Step [20/24], Loss: 19.4365\n",
      "Epoch [24/50], Step [10/24], Loss: 18.5795\n",
      "Epoch [24/50], Step [20/24], Loss: 19.3457\n",
      "Epoch [25/50], Step [10/24], Loss: 18.5161\n",
      "Epoch [25/50], Step [20/24], Loss: 19.5702\n",
      "Epoch [26/50], Step [10/24], Loss: 18.4529\n",
      "Epoch [26/50], Step [20/24], Loss: 19.3672\n",
      "Epoch [27/50], Step [10/24], Loss: 18.4086\n",
      "Epoch [27/50], Step [20/24], Loss: 19.3816\n",
      "Epoch [28/50], Step [10/24], Loss: 18.7396\n",
      "Epoch [28/50], Step [20/24], Loss: 19.4528\n",
      "Epoch [29/50], Step [10/24], Loss: 18.5367\n",
      "Epoch [29/50], Step [20/24], Loss: 19.4926\n",
      "Epoch [30/50], Step [10/24], Loss: 18.6105\n",
      "Epoch [30/50], Step [20/24], Loss: 19.4422\n",
      "Epoch [31/50], Step [10/24], Loss: 18.5080\n",
      "Epoch [31/50], Step [20/24], Loss: 19.2699\n",
      "Epoch [32/50], Step [10/24], Loss: 18.6340\n",
      "Epoch [32/50], Step [20/24], Loss: 19.6712\n",
      "Epoch [33/50], Step [10/24], Loss: 18.5187\n",
      "Epoch [33/50], Step [20/24], Loss: 19.4289\n",
      "Epoch [34/50], Step [10/24], Loss: 18.5378\n",
      "Epoch [34/50], Step [20/24], Loss: 19.2440\n",
      "Epoch [35/50], Step [10/24], Loss: 18.4331\n",
      "Epoch [35/50], Step [20/24], Loss: 19.5081\n",
      "Epoch [36/50], Step [10/24], Loss: 18.5746\n",
      "Epoch [36/50], Step [20/24], Loss: 19.4396\n",
      "Epoch [37/50], Step [10/24], Loss: 18.5184\n",
      "Epoch [37/50], Step [20/24], Loss: 19.2039\n",
      "Epoch [38/50], Step [10/24], Loss: 18.5387\n",
      "Epoch [38/50], Step [20/24], Loss: 19.4386\n",
      "Epoch [39/50], Step [10/24], Loss: 18.4184\n",
      "Epoch [39/50], Step [20/24], Loss: 19.3218\n",
      "Epoch [40/50], Step [10/24], Loss: 18.5430\n",
      "Epoch [40/50], Step [20/24], Loss: 19.4590\n",
      "Epoch [41/50], Step [10/24], Loss: 18.4496\n",
      "Epoch [41/50], Step [20/24], Loss: 19.4360\n",
      "Epoch [42/50], Step [10/24], Loss: 18.5260\n",
      "Epoch [42/50], Step [20/24], Loss: 19.2147\n",
      "Epoch [43/50], Step [10/24], Loss: 18.5840\n",
      "Epoch [43/50], Step [20/24], Loss: 19.2260\n",
      "Epoch [44/50], Step [10/24], Loss: 18.4918\n",
      "Epoch [44/50], Step [20/24], Loss: 19.2718\n",
      "Epoch [45/50], Step [10/24], Loss: 18.5093\n",
      "Epoch [45/50], Step [20/24], Loss: 19.3143\n",
      "Epoch [46/50], Step [10/24], Loss: 18.3956\n",
      "Epoch [46/50], Step [20/24], Loss: 19.4850\n",
      "Epoch [47/50], Step [10/24], Loss: 18.4197\n",
      "Epoch [47/50], Step [20/24], Loss: 19.5892\n",
      "Epoch [48/50], Step [10/24], Loss: 18.5024\n",
      "Epoch [48/50], Step [20/24], Loss: 19.1198\n",
      "Epoch [49/50], Step [10/24], Loss: 18.3335\n",
      "Epoch [49/50], Step [20/24], Loss: 19.6039\n",
      "Epoch [50/50], Step [10/24], Loss: 18.4889\n",
      "Epoch [50/50], Step [20/24], Loss: 19.4438\n"
     ]
    }
   ],
   "source": [
    "# 定义VAE模型结构\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.log_var = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "        # 定义解码器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mean = self.mean(h)\n",
    "        log_var = self.log_var(h)\n",
    "        return mean, log_var\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var\n",
    "\n",
    "\n",
    "# 训练VAE模型\n",
    "input_size = mut_matrix.shape[1]\n",
    "hidden_size = 256\n",
    "latent_size = 64\n",
    "lr = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "model = VAE(input_size, hidden_size, latent_size)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.BCELoss(reduction='sum')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        reconstruction_loss = loss_func(x_hat, x)\n",
    "        loss = kl_divergence + reconstruction_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()/batch_size:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取VAE模型的Z表达\n",
    "z_matrix = np.zeros((len(valid_set), latent_size))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, x in enumerate(valid_loader):\n",
    "        z_mean, _ = model.encode(x[0])\n",
    "        z_matrix[i*batch_size:(i+1)*batch_size] = z_mean.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在Z表达的基础上构建分类器模型进行分类\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i],dtype=torch.float64))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i],dtype=torch.float64))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size,dtype=torch.float64))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类器模型结构和训练过程\n",
    "input_size = latent_size\n",
    "hidden_sizes = [32, 16]\n",
    "output_size = 1\n",
    "lr = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "model = Classifier(input_size, hidden_sizes, output_size)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/6], Loss: 0.7360\n",
      "Epoch [1/50], Step [20/6], Loss: 0.6556\n",
      "Epoch [1/50], Step [30/6], Loss: 0.6479\n",
      "Epoch [1/50], Step [40/6], Loss: 0.6431\n",
      "Epoch [1/50], Step [50/6], Loss: 0.6412\n",
      "Epoch [1/50], Step [60/6], Loss: 0.6372\n",
      "Epoch [1/50], Step [70/6], Loss: 0.7604\n",
      "Epoch [1/50], Step [80/6], Loss: 0.7590\n",
      "Epoch [1/50], Step [90/6], Loss: 0.6298\n",
      "Epoch [1/50], Step [100/6], Loss: 0.7696\n",
      "Epoch [1/50], Step [110/6], Loss: 0.6190\n",
      "Epoch [1/50], Step [120/6], Loss: 0.6071\n",
      "Epoch [1/50], Step [130/6], Loss: 0.7803\n",
      "Epoch [1/50], Step [140/6], Loss: 0.8048\n",
      "Epoch [1/50], Step [150/6], Loss: 0.6025\n",
      "Epoch [1/50], Step [160/6], Loss: 0.7947\n",
      "Epoch [1/50], Step [170/6], Loss: 0.6063\n",
      "Epoch [1/50], Step [180/6], Loss: 0.5993\n",
      "Epoch [1/50], Step [190/6], Loss: 0.6087\n",
      "Epoch [1/50], Step [200/6], Loss: 0.6095\n",
      "Epoch [1/50], Step [210/6], Loss: 0.7799\n",
      "Epoch [1/50], Step [220/6], Loss: 0.7756\n",
      "Epoch [1/50], Step [230/6], Loss: 0.6250\n",
      "Epoch [1/50], Step [240/6], Loss: 0.6291\n",
      "Epoch [1/50], Step [250/6], Loss: 0.6305\n",
      "Epoch [1/50], Step [260/6], Loss: 0.7615\n",
      "Epoch [1/50], Step [270/6], Loss: 0.6283\n",
      "Epoch [1/50], Step [280/6], Loss: 0.6211\n",
      "Epoch [1/50], Step [290/6], Loss: 0.7591\n",
      "Epoch [1/50], Step [300/6], Loss: 0.6263\n",
      "Epoch [1/50], Step [310/6], Loss: 0.7660\n",
      "Epoch [1/50], Step [320/6], Loss: 0.6253\n",
      "Epoch [1/50], Step [330/6], Loss: 0.6142\n",
      "Epoch [1/50], Step [340/6], Loss: 0.6198\n",
      "Epoch [1/50], Step [350/6], Loss: 0.6135\n",
      "Epoch [1/50], Step [360/6], Loss: 0.6031\n",
      "Epoch [1/50], Step [370/6], Loss: 0.6007\n",
      "Epoch [2/50], Step [10/6], Loss: 0.7916\n",
      "Epoch [2/50], Step [20/6], Loss: 0.6082\n",
      "Epoch [2/50], Step [30/6], Loss: 0.6027\n",
      "Epoch [2/50], Step [40/6], Loss: 0.5827\n",
      "Epoch [2/50], Step [50/6], Loss: 0.6021\n",
      "Epoch [2/50], Step [60/6], Loss: 0.5992\n",
      "Epoch [2/50], Step [70/6], Loss: 0.8035\n",
      "Epoch [2/50], Step [80/6], Loss: 0.8129\n",
      "Epoch [2/50], Step [90/6], Loss: 0.5844\n",
      "Epoch [2/50], Step [100/6], Loss: 0.8097\n",
      "Epoch [2/50], Step [110/6], Loss: 0.5854\n",
      "Epoch [2/50], Step [120/6], Loss: 0.5599\n",
      "Epoch [2/50], Step [130/6], Loss: 0.8185\n",
      "Epoch [2/50], Step [140/6], Loss: 0.8698\n",
      "Epoch [2/50], Step [150/6], Loss: 0.5710\n",
      "Epoch [2/50], Step [160/6], Loss: 0.8337\n",
      "Epoch [2/50], Step [170/6], Loss: 0.5772\n",
      "Epoch [2/50], Step [180/6], Loss: 0.5608\n",
      "Epoch [2/50], Step [190/6], Loss: 0.5808\n",
      "Epoch [2/50], Step [200/6], Loss: 0.5810\n",
      "Epoch [2/50], Step [210/6], Loss: 0.8133\n",
      "Epoch [2/50], Step [220/6], Loss: 0.8111\n",
      "Epoch [2/50], Step [230/6], Loss: 0.5969\n",
      "Epoch [2/50], Step [240/6], Loss: 0.6005\n",
      "Epoch [2/50], Step [250/6], Loss: 0.6019\n",
      "Epoch [2/50], Step [260/6], Loss: 0.7944\n",
      "Epoch [2/50], Step [270/6], Loss: 0.5979\n",
      "Epoch [2/50], Step [280/6], Loss: 0.6094\n",
      "Epoch [2/50], Step [290/6], Loss: 0.7892\n",
      "Epoch [2/50], Step [300/6], Loss: 0.5992\n",
      "Epoch [2/50], Step [310/6], Loss: 0.8003\n",
      "Epoch [2/50], Step [320/6], Loss: 0.5993\n",
      "Epoch [2/50], Step [330/6], Loss: 0.5926\n",
      "Epoch [2/50], Step [340/6], Loss: 0.5945\n",
      "Epoch [2/50], Step [350/6], Loss: 0.5900\n",
      "Epoch [2/50], Step [360/6], Loss: 0.5820\n",
      "Epoch [2/50], Step [370/6], Loss: 0.5800\n",
      "Epoch [3/50], Step [10/6], Loss: 0.8169\n",
      "Epoch [3/50], Step [20/6], Loss: 0.5874\n",
      "Epoch [3/50], Step [30/6], Loss: 0.5821\n",
      "Epoch [3/50], Step [40/6], Loss: 0.5641\n",
      "Epoch [3/50], Step [50/6], Loss: 0.5839\n",
      "Epoch [3/50], Step [60/6], Loss: 0.5809\n",
      "Epoch [3/50], Step [70/6], Loss: 0.8262\n",
      "Epoch [3/50], Step [80/6], Loss: 0.8410\n",
      "Epoch [3/50], Step [90/6], Loss: 0.5628\n",
      "Epoch [3/50], Step [100/6], Loss: 0.8323\n",
      "Epoch [3/50], Step [110/6], Loss: 0.5653\n",
      "Epoch [3/50], Step [120/6], Loss: 0.5352\n",
      "Epoch [3/50], Step [130/6], Loss: 0.8395\n",
      "Epoch [3/50], Step [140/6], Loss: 0.9039\n",
      "Epoch [3/50], Step [150/6], Loss: 0.5540\n",
      "Epoch [3/50], Step [160/6], Loss: 0.8573\n",
      "Epoch [3/50], Step [170/6], Loss: 0.5617\n",
      "Epoch [3/50], Step [180/6], Loss: 0.5402\n",
      "Epoch [3/50], Step [190/6], Loss: 0.5662\n",
      "Epoch [3/50], Step [200/6], Loss: 0.5666\n",
      "Epoch [3/50], Step [210/6], Loss: 0.8304\n",
      "Epoch [3/50], Step [220/6], Loss: 0.8293\n",
      "Epoch [3/50], Step [230/6], Loss: 0.5785\n",
      "Epoch [3/50], Step [240/6], Loss: 0.5862\n",
      "Epoch [3/50], Step [250/6], Loss: 0.5873\n",
      "Epoch [3/50], Step [260/6], Loss: 0.8114\n",
      "Epoch [3/50], Step [270/6], Loss: 0.5833\n",
      "Epoch [3/50], Step [280/6], Loss: 0.5761\n",
      "Epoch [3/50], Step [290/6], Loss: 0.8040\n",
      "Epoch [3/50], Step [300/6], Loss: 0.5860\n",
      "Epoch [3/50], Step [310/6], Loss: 0.8176\n",
      "Epoch [3/50], Step [320/6], Loss: 0.5886\n",
      "Epoch [3/50], Step [330/6], Loss: 0.5699\n",
      "Epoch [3/50], Step [340/6], Loss: 0.5826\n",
      "Epoch [3/50], Step [350/6], Loss: 0.5783\n",
      "Epoch [3/50], Step [360/6], Loss: 0.5703\n",
      "Epoch [3/50], Step [370/6], Loss: 0.5697\n",
      "Epoch [4/50], Step [10/6], Loss: 0.8283\n",
      "Epoch [4/50], Step [20/6], Loss: 0.5753\n",
      "Epoch [4/50], Step [30/6], Loss: 0.5690\n",
      "Epoch [4/50], Step [40/6], Loss: 0.5411\n",
      "Epoch [4/50], Step [50/6], Loss: 0.5741\n",
      "Epoch [4/50], Step [60/6], Loss: 0.5717\n",
      "Epoch [4/50], Step [70/6], Loss: 0.8396\n",
      "Epoch [4/50], Step [80/6], Loss: 0.8598\n",
      "Epoch [4/50], Step [90/6], Loss: 0.5471\n",
      "Epoch [4/50], Step [100/6], Loss: 0.8448\n",
      "Epoch [4/50], Step [110/6], Loss: 0.5541\n",
      "Epoch [4/50], Step [120/6], Loss: 0.5180\n",
      "Epoch [4/50], Step [130/6], Loss: 0.8495\n",
      "Epoch [4/50], Step [140/6], Loss: 0.9313\n",
      "Epoch [4/50], Step [150/6], Loss: 0.5460\n",
      "Epoch [4/50], Step [160/6], Loss: 0.8695\n",
      "Epoch [4/50], Step [170/6], Loss: 0.5539\n",
      "Epoch [4/50], Step [180/6], Loss: 0.5263\n",
      "Epoch [4/50], Step [190/6], Loss: 0.5595\n",
      "Epoch [4/50], Step [200/6], Loss: 0.5578\n",
      "Epoch [4/50], Step [210/6], Loss: 0.8410\n",
      "Epoch [4/50], Step [220/6], Loss: 0.8415\n",
      "Epoch [4/50], Step [230/6], Loss: 0.5742\n",
      "Epoch [4/50], Step [240/6], Loss: 0.5825\n",
      "Epoch [4/50], Step [250/6], Loss: 0.5823\n",
      "Epoch [4/50], Step [260/6], Loss: 0.8174\n",
      "Epoch [4/50], Step [270/6], Loss: 0.5854\n",
      "Epoch [4/50], Step [280/6], Loss: 0.5978\n",
      "Epoch [4/50], Step [290/6], Loss: 0.8013\n",
      "Epoch [4/50], Step [300/6], Loss: 0.5857\n",
      "Epoch [4/50], Step [310/6], Loss: 0.8235\n",
      "Epoch [4/50], Step [320/6], Loss: 0.5844\n",
      "Epoch [4/50], Step [330/6], Loss: 0.5741\n",
      "Epoch [4/50], Step [340/6], Loss: 0.5751\n",
      "Epoch [4/50], Step [350/6], Loss: 0.5715\n",
      "Epoch [4/50], Step [360/6], Loss: 0.5633\n",
      "Epoch [4/50], Step [370/6], Loss: 0.5641\n",
      "Epoch [5/50], Step [10/6], Loss: 0.8346\n",
      "Epoch [5/50], Step [20/6], Loss: 0.5692\n",
      "Epoch [5/50], Step [30/6], Loss: 0.5607\n",
      "Epoch [5/50], Step [40/6], Loss: 0.5468\n",
      "Epoch [5/50], Step [50/6], Loss: 0.5676\n",
      "Epoch [5/50], Step [60/6], Loss: 0.5665\n",
      "Epoch [5/50], Step [70/6], Loss: 0.8486\n",
      "Epoch [5/50], Step [80/6], Loss: 0.8692\n",
      "Epoch [5/50], Step [90/6], Loss: 0.5427\n",
      "Epoch [5/50], Step [100/6], Loss: 0.8519\n",
      "Epoch [5/50], Step [110/6], Loss: 0.5475\n",
      "Epoch [5/50], Step [120/6], Loss: 0.5084\n",
      "Epoch [5/50], Step [130/6], Loss: 0.8549\n",
      "Epoch [5/50], Step [140/6], Loss: 0.9430\n",
      "Epoch [5/50], Step [150/6], Loss: 0.5410\n",
      "Epoch [5/50], Step [160/6], Loss: 0.8771\n",
      "Epoch [5/50], Step [170/6], Loss: 0.5492\n",
      "Epoch [5/50], Step [180/6], Loss: 0.5192\n",
      "Epoch [5/50], Step [190/6], Loss: 0.5552\n",
      "Epoch [5/50], Step [200/6], Loss: 0.5529\n",
      "Epoch [5/50], Step [210/6], Loss: 0.8469\n",
      "Epoch [5/50], Step [220/6], Loss: 0.8479\n",
      "Epoch [5/50], Step [230/6], Loss: 0.5642\n",
      "Epoch [5/50], Step [240/6], Loss: 0.5753\n",
      "Epoch [5/50], Step [250/6], Loss: 0.5753\n",
      "Epoch [5/50], Step [260/6], Loss: 0.8260\n",
      "Epoch [5/50], Step [270/6], Loss: 0.5679\n",
      "Epoch [5/50], Step [280/6], Loss: 0.5532\n",
      "Epoch [5/50], Step [290/6], Loss: 0.8146\n",
      "Epoch [5/50], Step [300/6], Loss: 0.5741\n",
      "Epoch [5/50], Step [310/6], Loss: 0.8323\n",
      "Epoch [5/50], Step [320/6], Loss: 0.5806\n",
      "Epoch [5/50], Step [330/6], Loss: 0.5519\n",
      "Epoch [5/50], Step [340/6], Loss: 0.5728\n",
      "Epoch [5/50], Step [350/6], Loss: 0.5694\n",
      "Epoch [5/50], Step [360/6], Loss: 0.5594\n",
      "Epoch [5/50], Step [370/6], Loss: 0.5625\n",
      "Epoch [6/50], Step [10/6], Loss: 0.8357\n",
      "Epoch [6/50], Step [20/6], Loss: 0.5634\n",
      "Epoch [6/50], Step [30/6], Loss: 0.5553\n",
      "Epoch [6/50], Step [40/6], Loss: 0.5279\n",
      "Epoch [6/50], Step [50/6], Loss: 0.5653\n",
      "Epoch [6/50], Step [60/6], Loss: 0.5648\n",
      "Epoch [6/50], Step [70/6], Loss: 0.8521\n",
      "Epoch [6/50], Step [80/6], Loss: 0.8794\n",
      "Epoch [6/50], Step [90/6], Loss: 0.5338\n",
      "Epoch [6/50], Step [100/6], Loss: 0.8558\n",
      "Epoch [6/50], Step [110/6], Loss: 0.5441\n",
      "Epoch [6/50], Step [120/6], Loss: 0.5034\n",
      "Epoch [6/50], Step [130/6], Loss: 0.8567\n",
      "Epoch [6/50], Step [140/6], Loss: 0.9513\n",
      "Epoch [6/50], Step [150/6], Loss: 0.5401\n",
      "Epoch [6/50], Step [160/6], Loss: 0.8805\n",
      "Epoch [6/50], Step [170/6], Loss: 0.5473\n",
      "Epoch [6/50], Step [180/6], Loss: 0.5141\n",
      "Epoch [6/50], Step [190/6], Loss: 0.5547\n",
      "Epoch [6/50], Step [200/6], Loss: 0.5499\n",
      "Epoch [6/50], Step [210/6], Loss: 0.8501\n",
      "Epoch [6/50], Step [220/6], Loss: 0.8522\n",
      "Epoch [6/50], Step [230/6], Loss: 0.5604\n",
      "Epoch [6/50], Step [240/6], Loss: 0.5728\n",
      "Epoch [6/50], Step [250/6], Loss: 0.5720\n",
      "Epoch [6/50], Step [260/6], Loss: 0.8293\n",
      "Epoch [6/50], Step [270/6], Loss: 0.5631\n",
      "Epoch [6/50], Step [280/6], Loss: 0.5498\n",
      "Epoch [6/50], Step [290/6], Loss: 0.8162\n",
      "Epoch [6/50], Step [300/6], Loss: 0.5702\n",
      "Epoch [6/50], Step [310/6], Loss: 0.8376\n",
      "Epoch [6/50], Step [320/6], Loss: 0.5789\n",
      "Epoch [6/50], Step [330/6], Loss: 0.5455\n",
      "Epoch [6/50], Step [340/6], Loss: 0.5696\n",
      "Epoch [6/50], Step [350/6], Loss: 0.5657\n",
      "Epoch [6/50], Step [360/6], Loss: 0.5579\n",
      "Epoch [6/50], Step [370/6], Loss: 0.5600\n",
      "Epoch [7/50], Step [10/6], Loss: 0.8370\n",
      "Epoch [7/50], Step [20/6], Loss: 0.5609\n",
      "Epoch [7/50], Step [30/6], Loss: 0.5529\n",
      "Epoch [7/50], Step [40/6], Loss: 0.5255\n",
      "Epoch [7/50], Step [50/6], Loss: 0.5624\n",
      "Epoch [7/50], Step [60/6], Loss: 0.5635\n",
      "Epoch [7/50], Step [70/6], Loss: 0.8566\n",
      "Epoch [7/50], Step [80/6], Loss: 0.8840\n",
      "Epoch [7/50], Step [90/6], Loss: 0.5293\n",
      "Epoch [7/50], Step [100/6], Loss: 0.8602\n",
      "Epoch [7/50], Step [110/6], Loss: 0.5426\n",
      "Epoch [7/50], Step [120/6], Loss: 0.4987\n",
      "Epoch [7/50], Step [130/6], Loss: 0.8582\n",
      "Epoch [7/50], Step [140/6], Loss: 0.9575\n",
      "Epoch [7/50], Step [150/6], Loss: 0.5395\n",
      "Epoch [7/50], Step [160/6], Loss: 0.8837\n",
      "Epoch [7/50], Step [170/6], Loss: 0.5453\n",
      "Epoch [7/50], Step [180/6], Loss: 0.5099\n",
      "Epoch [7/50], Step [190/6], Loss: 0.5548\n",
      "Epoch [7/50], Step [200/6], Loss: 0.5492\n",
      "Epoch [7/50], Step [210/6], Loss: 0.8517\n",
      "Epoch [7/50], Step [220/6], Loss: 0.8542\n",
      "Epoch [7/50], Step [230/6], Loss: 0.5581\n",
      "Epoch [7/50], Step [240/6], Loss: 0.5723\n",
      "Epoch [7/50], Step [250/6], Loss: 0.5709\n",
      "Epoch [7/50], Step [260/6], Loss: 0.8303\n",
      "Epoch [7/50], Step [270/6], Loss: 0.5608\n",
      "Epoch [7/50], Step [280/6], Loss: 0.5451\n",
      "Epoch [7/50], Step [290/6], Loss: 0.8147\n",
      "Epoch [7/50], Step [300/6], Loss: 0.5683\n",
      "Epoch [7/50], Step [310/6], Loss: 0.8396\n",
      "Epoch [7/50], Step [320/6], Loss: 0.5795\n",
      "Epoch [7/50], Step [330/6], Loss: 0.5429\n",
      "Epoch [7/50], Step [340/6], Loss: 0.5687\n",
      "Epoch [7/50], Step [350/6], Loss: 0.5651\n",
      "Epoch [7/50], Step [360/6], Loss: 0.5572\n",
      "Epoch [7/50], Step [370/6], Loss: 0.5604\n",
      "Epoch [8/50], Step [10/6], Loss: 0.8355\n",
      "Epoch [8/50], Step [20/6], Loss: 0.5591\n",
      "Epoch [8/50], Step [30/6], Loss: 0.5506\n",
      "Epoch [8/50], Step [40/6], Loss: 0.5255\n",
      "Epoch [8/50], Step [50/6], Loss: 0.5619\n",
      "Epoch [8/50], Step [60/6], Loss: 0.5641\n",
      "Epoch [8/50], Step [70/6], Loss: 0.8569\n",
      "Epoch [8/50], Step [80/6], Loss: 0.8843\n",
      "Epoch [8/50], Step [90/6], Loss: 0.5285\n",
      "Epoch [8/50], Step [100/6], Loss: 0.8607\n",
      "Epoch [8/50], Step [110/6], Loss: 0.5397\n",
      "Epoch [8/50], Step [120/6], Loss: 0.4938\n",
      "Epoch [8/50], Step [130/6], Loss: 0.8575\n",
      "Epoch [8/50], Step [140/6], Loss: 0.9639\n",
      "Epoch [8/50], Step [150/6], Loss: 0.5398\n",
      "Epoch [8/50], Step [160/6], Loss: 0.8854\n",
      "Epoch [8/50], Step [170/6], Loss: 0.5449\n",
      "Epoch [8/50], Step [180/6], Loss: 0.5062\n",
      "Epoch [8/50], Step [190/6], Loss: 0.5558\n",
      "Epoch [8/50], Step [200/6], Loss: 0.5479\n",
      "Epoch [8/50], Step [210/6], Loss: 0.8535\n",
      "Epoch [8/50], Step [220/6], Loss: 0.8561\n",
      "Epoch [8/50], Step [230/6], Loss: 0.5578\n",
      "Epoch [8/50], Step [240/6], Loss: 0.5732\n",
      "Epoch [8/50], Step [250/6], Loss: 0.5700\n",
      "Epoch [8/50], Step [260/6], Loss: 0.8307\n",
      "Epoch [8/50], Step [270/6], Loss: 0.5609\n",
      "Epoch [8/50], Step [280/6], Loss: 0.5312\n",
      "Epoch [8/50], Step [290/6], Loss: 0.8123\n",
      "Epoch [8/50], Step [300/6], Loss: 0.5671\n",
      "Epoch [8/50], Step [310/6], Loss: 0.8414\n",
      "Epoch [8/50], Step [320/6], Loss: 0.5809\n",
      "Epoch [8/50], Step [330/6], Loss: 0.5378\n",
      "Epoch [8/50], Step [340/6], Loss: 0.5679\n",
      "Epoch [8/50], Step [350/6], Loss: 0.5642\n",
      "Epoch [8/50], Step [360/6], Loss: 0.5565\n",
      "Epoch [8/50], Step [370/6], Loss: 0.5603\n",
      "Epoch [9/50], Step [10/6], Loss: 0.8336\n",
      "Epoch [9/50], Step [20/6], Loss: 0.5578\n",
      "Epoch [9/50], Step [30/6], Loss: 0.5487\n",
      "Epoch [9/50], Step [40/6], Loss: 0.5183\n",
      "Epoch [9/50], Step [50/6], Loss: 0.5608\n",
      "Epoch [9/50], Step [60/6], Loss: 0.5646\n",
      "Epoch [9/50], Step [70/6], Loss: 0.8577\n",
      "Epoch [9/50], Step [80/6], Loss: 0.8868\n",
      "Epoch [9/50], Step [90/6], Loss: 0.5256\n",
      "Epoch [9/50], Step [100/6], Loss: 0.8622\n",
      "Epoch [9/50], Step [110/6], Loss: 0.5376\n",
      "Epoch [9/50], Step [120/6], Loss: 0.4890\n",
      "Epoch [9/50], Step [130/6], Loss: 0.8569\n",
      "Epoch [9/50], Step [140/6], Loss: 0.9704\n",
      "Epoch [9/50], Step [150/6], Loss: 0.5408\n",
      "Epoch [9/50], Step [160/6], Loss: 0.8860\n",
      "Epoch [9/50], Step [170/6], Loss: 0.5445\n",
      "Epoch [9/50], Step [180/6], Loss: 0.5027\n",
      "Epoch [9/50], Step [190/6], Loss: 0.5578\n",
      "Epoch [9/50], Step [200/6], Loss: 0.5465\n",
      "Epoch [9/50], Step [210/6], Loss: 0.8531\n",
      "Epoch [9/50], Step [220/6], Loss: 0.8579\n",
      "Epoch [9/50], Step [230/6], Loss: 0.5568\n",
      "Epoch [9/50], Step [240/6], Loss: 0.5742\n",
      "Epoch [9/50], Step [250/6], Loss: 0.5691\n",
      "Epoch [9/50], Step [260/6], Loss: 0.8314\n",
      "Epoch [9/50], Step [270/6], Loss: 0.5573\n",
      "Epoch [9/50], Step [280/6], Loss: 0.5234\n",
      "Epoch [9/50], Step [290/6], Loss: 0.8096\n",
      "Epoch [9/50], Step [300/6], Loss: 0.5662\n",
      "Epoch [9/50], Step [310/6], Loss: 0.8424\n",
      "Epoch [9/50], Step [320/6], Loss: 0.5824\n",
      "Epoch [9/50], Step [330/6], Loss: 0.5350\n",
      "Epoch [9/50], Step [340/6], Loss: 0.5671\n",
      "Epoch [9/50], Step [350/6], Loss: 0.5634\n",
      "Epoch [9/50], Step [360/6], Loss: 0.5559\n",
      "Epoch [9/50], Step [370/6], Loss: 0.5608\n",
      "Epoch [10/50], Step [10/6], Loss: 0.8312\n",
      "Epoch [10/50], Step [20/6], Loss: 0.5570\n",
      "Epoch [10/50], Step [30/6], Loss: 0.5475\n",
      "Epoch [10/50], Step [40/6], Loss: 0.5145\n",
      "Epoch [10/50], Step [50/6], Loss: 0.5606\n",
      "Epoch [10/50], Step [60/6], Loss: 0.5653\n",
      "Epoch [10/50], Step [70/6], Loss: 0.8569\n",
      "Epoch [10/50], Step [80/6], Loss: 0.8870\n",
      "Epoch [10/50], Step [90/6], Loss: 0.5237\n",
      "Epoch [10/50], Step [100/6], Loss: 0.8630\n",
      "Epoch [10/50], Step [110/6], Loss: 0.5364\n",
      "Epoch [10/50], Step [120/6], Loss: 0.4835\n",
      "Epoch [10/50], Step [130/6], Loss: 0.8562\n",
      "Epoch [10/50], Step [140/6], Loss: 0.9796\n",
      "Epoch [10/50], Step [150/6], Loss: 0.5417\n",
      "Epoch [10/50], Step [160/6], Loss: 0.8870\n",
      "Epoch [10/50], Step [170/6], Loss: 0.5445\n",
      "Epoch [10/50], Step [180/6], Loss: 0.4980\n",
      "Epoch [10/50], Step [190/6], Loss: 0.5599\n",
      "Epoch [10/50], Step [200/6], Loss: 0.5459\n",
      "Epoch [10/50], Step [210/6], Loss: 0.8536\n",
      "Epoch [10/50], Step [220/6], Loss: 0.8594\n",
      "Epoch [10/50], Step [230/6], Loss: 0.5559\n",
      "Epoch [10/50], Step [240/6], Loss: 0.5766\n",
      "Epoch [10/50], Step [250/6], Loss: 0.5686\n",
      "Epoch [10/50], Step [260/6], Loss: 0.8310\n",
      "Epoch [10/50], Step [270/6], Loss: 0.5558\n",
      "Epoch [10/50], Step [280/6], Loss: 0.5202\n",
      "Epoch [10/50], Step [290/6], Loss: 0.8046\n",
      "Epoch [10/50], Step [300/6], Loss: 0.5652\n",
      "Epoch [10/50], Step [310/6], Loss: 0.8432\n",
      "Epoch [10/50], Step [320/6], Loss: 0.5854\n",
      "Epoch [10/50], Step [330/6], Loss: 0.5315\n",
      "Epoch [10/50], Step [340/6], Loss: 0.5669\n",
      "Epoch [10/50], Step [350/6], Loss: 0.5629\n",
      "Epoch [10/50], Step [360/6], Loss: 0.5564\n",
      "Epoch [10/50], Step [370/6], Loss: 0.5619\n",
      "Epoch [11/50], Step [10/6], Loss: 0.8271\n",
      "Epoch [11/50], Step [20/6], Loss: 0.5558\n",
      "Epoch [11/50], Step [30/6], Loss: 0.5458\n",
      "Epoch [11/50], Step [40/6], Loss: 0.5147\n",
      "Epoch [11/50], Step [50/6], Loss: 0.5598\n",
      "Epoch [11/50], Step [60/6], Loss: 0.5671\n",
      "Epoch [11/50], Step [70/6], Loss: 0.8566\n",
      "Epoch [11/50], Step [80/6], Loss: 0.8874\n",
      "Epoch [11/50], Step [90/6], Loss: 0.5218\n",
      "Epoch [11/50], Step [100/6], Loss: 0.8651\n",
      "Epoch [11/50], Step [110/6], Loss: 0.5344\n",
      "Epoch [11/50], Step [120/6], Loss: 0.4784\n",
      "Epoch [11/50], Step [130/6], Loss: 0.8551\n",
      "Epoch [11/50], Step [140/6], Loss: 0.9874\n",
      "Epoch [11/50], Step [150/6], Loss: 0.5429\n",
      "Epoch [11/50], Step [160/6], Loss: 0.8885\n",
      "Epoch [11/50], Step [170/6], Loss: 0.5444\n",
      "Epoch [11/50], Step [180/6], Loss: 0.4933\n",
      "Epoch [11/50], Step [190/6], Loss: 0.5628\n",
      "Epoch [11/50], Step [200/6], Loss: 0.5456\n",
      "Epoch [11/50], Step [210/6], Loss: 0.8546\n",
      "Epoch [11/50], Step [220/6], Loss: 0.8613\n",
      "Epoch [11/50], Step [230/6], Loss: 0.5552\n",
      "Epoch [11/50], Step [240/6], Loss: 0.5795\n",
      "Epoch [11/50], Step [250/6], Loss: 0.5679\n",
      "Epoch [11/50], Step [260/6], Loss: 0.8306\n",
      "Epoch [11/50], Step [270/6], Loss: 0.5532\n",
      "Epoch [11/50], Step [280/6], Loss: 0.5143\n",
      "Epoch [11/50], Step [290/6], Loss: 0.8000\n",
      "Epoch [11/50], Step [300/6], Loss: 0.5638\n",
      "Epoch [11/50], Step [310/6], Loss: 0.8433\n",
      "Epoch [11/50], Step [320/6], Loss: 0.5892\n",
      "Epoch [11/50], Step [330/6], Loss: 0.5285\n",
      "Epoch [11/50], Step [340/6], Loss: 0.5665\n",
      "Epoch [11/50], Step [350/6], Loss: 0.5631\n",
      "Epoch [11/50], Step [360/6], Loss: 0.5547\n",
      "Epoch [11/50], Step [370/6], Loss: 0.5638\n",
      "Epoch [12/50], Step [10/6], Loss: 0.8218\n",
      "Epoch [12/50], Step [20/6], Loss: 0.5540\n",
      "Epoch [12/50], Step [30/6], Loss: 0.5438\n",
      "Epoch [12/50], Step [40/6], Loss: 0.5142\n",
      "Epoch [12/50], Step [50/6], Loss: 0.5599\n",
      "Epoch [12/50], Step [60/6], Loss: 0.5693\n",
      "Epoch [12/50], Step [70/6], Loss: 0.8560\n",
      "Epoch [12/50], Step [80/6], Loss: 0.8863\n",
      "Epoch [12/50], Step [90/6], Loss: 0.5209\n",
      "Epoch [12/50], Step [100/6], Loss: 0.8659\n",
      "Epoch [12/50], Step [110/6], Loss: 0.5325\n",
      "Epoch [12/50], Step [120/6], Loss: 0.4739\n",
      "Epoch [12/50], Step [130/6], Loss: 0.8527\n",
      "Epoch [12/50], Step [140/6], Loss: 0.9936\n",
      "Epoch [12/50], Step [150/6], Loss: 0.5446\n",
      "Epoch [12/50], Step [160/6], Loss: 0.8893\n",
      "Epoch [12/50], Step [170/6], Loss: 0.5446\n",
      "Epoch [12/50], Step [180/6], Loss: 0.4893\n",
      "Epoch [12/50], Step [190/6], Loss: 0.5665\n",
      "Epoch [12/50], Step [200/6], Loss: 0.5450\n",
      "Epoch [12/50], Step [210/6], Loss: 0.8547\n",
      "Epoch [12/50], Step [220/6], Loss: 0.8629\n",
      "Epoch [12/50], Step [230/6], Loss: 0.5548\n",
      "Epoch [12/50], Step [240/6], Loss: 0.5838\n",
      "Epoch [12/50], Step [250/6], Loss: 0.5682\n",
      "Epoch [12/50], Step [260/6], Loss: 0.8289\n",
      "Epoch [12/50], Step [270/6], Loss: 0.5523\n",
      "Epoch [12/50], Step [280/6], Loss: 0.5075\n",
      "Epoch [12/50], Step [290/6], Loss: 0.7919\n",
      "Epoch [12/50], Step [300/6], Loss: 0.5633\n",
      "Epoch [12/50], Step [310/6], Loss: 0.8436\n",
      "Epoch [12/50], Step [320/6], Loss: 0.5947\n",
      "Epoch [12/50], Step [330/6], Loss: 0.5252\n",
      "Epoch [12/50], Step [340/6], Loss: 0.5665\n",
      "Epoch [12/50], Step [350/6], Loss: 0.5629\n",
      "Epoch [12/50], Step [360/6], Loss: 0.5547\n",
      "Epoch [12/50], Step [370/6], Loss: 0.5665\n",
      "Epoch [13/50], Step [10/6], Loss: 0.8154\n",
      "Epoch [13/50], Step [20/6], Loss: 0.5517\n",
      "Epoch [13/50], Step [30/6], Loss: 0.5409\n",
      "Epoch [13/50], Step [40/6], Loss: 0.5159\n",
      "Epoch [13/50], Step [50/6], Loss: 0.5587\n",
      "Epoch [13/50], Step [60/6], Loss: 0.5719\n",
      "Epoch [13/50], Step [70/6], Loss: 0.8558\n",
      "Epoch [13/50], Step [80/6], Loss: 0.8849\n",
      "Epoch [13/50], Step [90/6], Loss: 0.5194\n",
      "Epoch [13/50], Step [100/6], Loss: 0.8683\n",
      "Epoch [13/50], Step [110/6], Loss: 0.5297\n",
      "Epoch [13/50], Step [120/6], Loss: 0.4673\n",
      "Epoch [13/50], Step [130/6], Loss: 0.8508\n",
      "Epoch [13/50], Step [140/6], Loss: 1.0038\n",
      "Epoch [13/50], Step [150/6], Loss: 0.5462\n",
      "Epoch [13/50], Step [160/6], Loss: 0.8907\n",
      "Epoch [13/50], Step [170/6], Loss: 0.5448\n",
      "Epoch [13/50], Step [180/6], Loss: 0.4839\n",
      "Epoch [13/50], Step [190/6], Loss: 0.5710\n",
      "Epoch [13/50], Step [200/6], Loss: 0.5448\n",
      "Epoch [13/50], Step [210/6], Loss: 0.8551\n",
      "Epoch [13/50], Step [220/6], Loss: 0.8647\n",
      "Epoch [13/50], Step [230/6], Loss: 0.5538\n",
      "Epoch [13/50], Step [240/6], Loss: 0.5884\n",
      "Epoch [13/50], Step [250/6], Loss: 0.5672\n",
      "Epoch [13/50], Step [260/6], Loss: 0.8283\n",
      "Epoch [13/50], Step [270/6], Loss: 0.5512\n",
      "Epoch [13/50], Step [280/6], Loss: 0.5028\n",
      "Epoch [13/50], Step [290/6], Loss: 0.7811\n",
      "Epoch [13/50], Step [300/6], Loss: 0.5644\n",
      "Epoch [13/50], Step [310/6], Loss: 0.8447\n",
      "Epoch [13/50], Step [320/6], Loss: 0.6008\n",
      "Epoch [13/50], Step [330/6], Loss: 0.5214\n",
      "Epoch [13/50], Step [340/6], Loss: 0.5652\n",
      "Epoch [13/50], Step [350/6], Loss: 0.5628\n",
      "Epoch [13/50], Step [360/6], Loss: 0.5552\n",
      "Epoch [13/50], Step [370/6], Loss: 0.5682\n",
      "Epoch [14/50], Step [10/6], Loss: 0.8089\n",
      "Epoch [14/50], Step [20/6], Loss: 0.5502\n",
      "Epoch [14/50], Step [30/6], Loss: 0.5390\n",
      "Epoch [14/50], Step [40/6], Loss: 0.5192\n",
      "Epoch [14/50], Step [50/6], Loss: 0.5574\n",
      "Epoch [14/50], Step [60/6], Loss: 0.5742\n",
      "Epoch [14/50], Step [70/6], Loss: 0.8548\n",
      "Epoch [14/50], Step [80/6], Loss: 0.8813\n",
      "Epoch [14/50], Step [90/6], Loss: 0.5192\n",
      "Epoch [14/50], Step [100/6], Loss: 0.8705\n",
      "Epoch [14/50], Step [110/6], Loss: 0.5275\n",
      "Epoch [14/50], Step [120/6], Loss: 0.4609\n",
      "Epoch [14/50], Step [130/6], Loss: 0.8490\n",
      "Epoch [14/50], Step [140/6], Loss: 1.0136\n",
      "Epoch [14/50], Step [150/6], Loss: 0.5480\n",
      "Epoch [14/50], Step [160/6], Loss: 0.8917\n",
      "Epoch [14/50], Step [170/6], Loss: 0.5445\n",
      "Epoch [14/50], Step [180/6], Loss: 0.4783\n",
      "Epoch [14/50], Step [190/6], Loss: 0.5762\n",
      "Epoch [14/50], Step [200/6], Loss: 0.5436\n",
      "Epoch [14/50], Step [210/6], Loss: 0.8525\n",
      "Epoch [14/50], Step [220/6], Loss: 0.8625\n",
      "Epoch [14/50], Step [230/6], Loss: 0.5533\n",
      "Epoch [14/50], Step [240/6], Loss: 0.5939\n",
      "Epoch [14/50], Step [250/6], Loss: 0.5663\n",
      "Epoch [14/50], Step [260/6], Loss: 0.8287\n",
      "Epoch [14/50], Step [270/6], Loss: 0.5465\n",
      "Epoch [14/50], Step [280/6], Loss: 0.5004\n",
      "Epoch [14/50], Step [290/6], Loss: 0.7709\n",
      "Epoch [14/50], Step [300/6], Loss: 0.5631\n",
      "Epoch [14/50], Step [310/6], Loss: 0.8458\n",
      "Epoch [14/50], Step [320/6], Loss: 0.6068\n",
      "Epoch [14/50], Step [330/6], Loss: 0.5183\n",
      "Epoch [14/50], Step [340/6], Loss: 0.5638\n",
      "Epoch [14/50], Step [350/6], Loss: 0.5613\n",
      "Epoch [14/50], Step [360/6], Loss: 0.5570\n",
      "Epoch [14/50], Step [370/6], Loss: 0.5715\n",
      "Epoch [15/50], Step [10/6], Loss: 0.8006\n",
      "Epoch [15/50], Step [20/6], Loss: 0.5483\n",
      "Epoch [15/50], Step [30/6], Loss: 0.5373\n",
      "Epoch [15/50], Step [40/6], Loss: 0.5214\n",
      "Epoch [15/50], Step [50/6], Loss: 0.5562\n",
      "Epoch [15/50], Step [60/6], Loss: 0.5777\n",
      "Epoch [15/50], Step [70/6], Loss: 0.8520\n",
      "Epoch [15/50], Step [80/6], Loss: 0.8749\n",
      "Epoch [15/50], Step [90/6], Loss: 0.5201\n",
      "Epoch [15/50], Step [100/6], Loss: 0.8739\n",
      "Epoch [15/50], Step [110/6], Loss: 0.5249\n",
      "Epoch [15/50], Step [120/6], Loss: 0.4543\n",
      "Epoch [15/50], Step [130/6], Loss: 0.8472\n",
      "Epoch [15/50], Step [140/6], Loss: 1.0236\n",
      "Epoch [15/50], Step [150/6], Loss: 0.5500\n",
      "Epoch [15/50], Step [160/6], Loss: 0.8931\n",
      "Epoch [15/50], Step [170/6], Loss: 0.5438\n",
      "Epoch [15/50], Step [180/6], Loss: 0.4724\n",
      "Epoch [15/50], Step [190/6], Loss: 0.5830\n",
      "Epoch [15/50], Step [200/6], Loss: 0.5427\n",
      "Epoch [15/50], Step [210/6], Loss: 0.8514\n",
      "Epoch [15/50], Step [220/6], Loss: 0.8634\n",
      "Epoch [15/50], Step [230/6], Loss: 0.5536\n",
      "Epoch [15/50], Step [240/6], Loss: 0.6018\n",
      "Epoch [15/50], Step [250/6], Loss: 0.5648\n",
      "Epoch [15/50], Step [260/6], Loss: 0.8266\n",
      "Epoch [15/50], Step [270/6], Loss: 0.5452\n",
      "Epoch [15/50], Step [280/6], Loss: 0.4921\n",
      "Epoch [15/50], Step [290/6], Loss: 0.7580\n",
      "Epoch [15/50], Step [300/6], Loss: 0.5636\n",
      "Epoch [15/50], Step [310/6], Loss: 0.8456\n",
      "Epoch [15/50], Step [320/6], Loss: 0.6167\n",
      "Epoch [15/50], Step [330/6], Loss: 0.5138\n",
      "Epoch [15/50], Step [340/6], Loss: 0.5624\n",
      "Epoch [15/50], Step [350/6], Loss: 0.5623\n",
      "Epoch [15/50], Step [360/6], Loss: 0.5584\n",
      "Epoch [15/50], Step [370/6], Loss: 0.5764\n",
      "Epoch [16/50], Step [10/6], Loss: 0.7893\n",
      "Epoch [16/50], Step [20/6], Loss: 0.5463\n",
      "Epoch [16/50], Step [30/6], Loss: 0.5352\n",
      "Epoch [16/50], Step [40/6], Loss: 0.5254\n",
      "Epoch [16/50], Step [50/6], Loss: 0.5541\n",
      "Epoch [16/50], Step [60/6], Loss: 0.5820\n",
      "Epoch [16/50], Step [70/6], Loss: 0.8491\n",
      "Epoch [16/50], Step [80/6], Loss: 0.8713\n",
      "Epoch [16/50], Step [90/6], Loss: 0.5183\n",
      "Epoch [16/50], Step [100/6], Loss: 0.8775\n",
      "Epoch [16/50], Step [110/6], Loss: 0.5222\n",
      "Epoch [16/50], Step [120/6], Loss: 0.4441\n",
      "Epoch [16/50], Step [130/6], Loss: 0.8441\n",
      "Epoch [16/50], Step [140/6], Loss: 1.0402\n",
      "Epoch [16/50], Step [150/6], Loss: 0.5517\n",
      "Epoch [16/50], Step [160/6], Loss: 0.8944\n",
      "Epoch [16/50], Step [170/6], Loss: 0.5435\n",
      "Epoch [16/50], Step [180/6], Loss: 0.4641\n",
      "Epoch [16/50], Step [190/6], Loss: 0.5903\n",
      "Epoch [16/50], Step [200/6], Loss: 0.5414\n",
      "Epoch [16/50], Step [210/6], Loss: 0.8485\n",
      "Epoch [16/50], Step [220/6], Loss: 0.8618\n",
      "Epoch [16/50], Step [230/6], Loss: 0.5524\n",
      "Epoch [16/50], Step [240/6], Loss: 0.6096\n",
      "Epoch [16/50], Step [250/6], Loss: 0.5635\n",
      "Epoch [16/50], Step [260/6], Loss: 0.8274\n",
      "Epoch [16/50], Step [270/6], Loss: 0.5383\n",
      "Epoch [16/50], Step [280/6], Loss: 0.4890\n",
      "Epoch [16/50], Step [290/6], Loss: 0.7478\n",
      "Epoch [16/50], Step [300/6], Loss: 0.5606\n",
      "Epoch [16/50], Step [310/6], Loss: 0.8432\n",
      "Epoch [16/50], Step [320/6], Loss: 0.6244\n",
      "Epoch [16/50], Step [330/6], Loss: 0.5079\n",
      "Epoch [16/50], Step [340/6], Loss: 0.5607\n",
      "Epoch [16/50], Step [350/6], Loss: 0.5608\n",
      "Epoch [16/50], Step [360/6], Loss: 0.5580\n",
      "Epoch [16/50], Step [370/6], Loss: 0.5793\n",
      "Epoch [17/50], Step [10/6], Loss: 0.7783\n",
      "Epoch [17/50], Step [20/6], Loss: 0.5410\n",
      "Epoch [17/50], Step [30/6], Loss: 0.5312\n",
      "Epoch [17/50], Step [40/6], Loss: 0.5278\n",
      "Epoch [17/50], Step [50/6], Loss: 0.5513\n",
      "Epoch [17/50], Step [60/6], Loss: 0.5853\n",
      "Epoch [17/50], Step [70/6], Loss: 0.8459\n",
      "Epoch [17/50], Step [80/6], Loss: 0.8689\n",
      "Epoch [17/50], Step [90/6], Loss: 0.5167\n",
      "Epoch [17/50], Step [100/6], Loss: 0.8820\n",
      "Epoch [17/50], Step [110/6], Loss: 0.5186\n",
      "Epoch [17/50], Step [120/6], Loss: 0.4327\n",
      "Epoch [17/50], Step [130/6], Loss: 0.8419\n",
      "Epoch [17/50], Step [140/6], Loss: 1.0590\n",
      "Epoch [17/50], Step [150/6], Loss: 0.5526\n",
      "Epoch [17/50], Step [160/6], Loss: 0.8958\n",
      "Epoch [17/50], Step [170/6], Loss: 0.5427\n",
      "Epoch [17/50], Step [180/6], Loss: 0.4547\n",
      "Epoch [17/50], Step [190/6], Loss: 0.5973\n",
      "Epoch [17/50], Step [200/6], Loss: 0.5398\n",
      "Epoch [17/50], Step [210/6], Loss: 0.8452\n",
      "Epoch [17/50], Step [220/6], Loss: 0.8606\n",
      "Epoch [17/50], Step [230/6], Loss: 0.5508\n",
      "Epoch [17/50], Step [240/6], Loss: 0.6179\n",
      "Epoch [17/50], Step [250/6], Loss: 0.5630\n",
      "Epoch [17/50], Step [260/6], Loss: 0.8261\n",
      "Epoch [17/50], Step [270/6], Loss: 0.5335\n",
      "Epoch [17/50], Step [280/6], Loss: 0.4838\n",
      "Epoch [17/50], Step [290/6], Loss: 0.7355\n",
      "Epoch [17/50], Step [300/6], Loss: 0.5591\n",
      "Epoch [17/50], Step [310/6], Loss: 0.8412\n",
      "Epoch [17/50], Step [320/6], Loss: 0.6343\n",
      "Epoch [17/50], Step [330/6], Loss: 0.5031\n",
      "Epoch [17/50], Step [340/6], Loss: 0.5588\n",
      "Epoch [17/50], Step [350/6], Loss: 0.5610\n",
      "Epoch [17/50], Step [360/6], Loss: 0.5622\n",
      "Epoch [17/50], Step [370/6], Loss: 0.5842\n",
      "Epoch [18/50], Step [10/6], Loss: 0.7678\n",
      "Epoch [18/50], Step [20/6], Loss: 0.5407\n",
      "Epoch [18/50], Step [30/6], Loss: 0.5308\n",
      "Epoch [18/50], Step [40/6], Loss: 0.5246\n",
      "Epoch [18/50], Step [50/6], Loss: 0.5494\n",
      "Epoch [18/50], Step [60/6], Loss: 0.5879\n",
      "Epoch [18/50], Step [70/6], Loss: 0.8393\n",
      "Epoch [18/50], Step [80/6], Loss: 0.8698\n",
      "Epoch [18/50], Step [90/6], Loss: 0.5115\n",
      "Epoch [18/50], Step [100/6], Loss: 0.8825\n",
      "Epoch [18/50], Step [110/6], Loss: 0.5196\n",
      "Epoch [18/50], Step [120/6], Loss: 0.4215\n",
      "Epoch [18/50], Step [130/6], Loss: 0.8387\n",
      "Epoch [18/50], Step [140/6], Loss: 1.0767\n",
      "Epoch [18/50], Step [150/6], Loss: 0.5528\n",
      "Epoch [18/50], Step [160/6], Loss: 0.8948\n",
      "Epoch [18/50], Step [170/6], Loss: 0.5426\n",
      "Epoch [18/50], Step [180/6], Loss: 0.4446\n",
      "Epoch [18/50], Step [190/6], Loss: 0.6035\n",
      "Epoch [18/50], Step [200/6], Loss: 0.5382\n",
      "Epoch [18/50], Step [210/6], Loss: 0.8395\n",
      "Epoch [18/50], Step [220/6], Loss: 0.8564\n",
      "Epoch [18/50], Step [230/6], Loss: 0.5475\n",
      "Epoch [18/50], Step [240/6], Loss: 0.6244\n",
      "Epoch [18/50], Step [250/6], Loss: 0.5596\n",
      "Epoch [18/50], Step [260/6], Loss: 0.8285\n",
      "Epoch [18/50], Step [270/6], Loss: 0.5293\n",
      "Epoch [18/50], Step [280/6], Loss: 0.4832\n",
      "Epoch [18/50], Step [290/6], Loss: 0.7271\n",
      "Epoch [18/50], Step [300/6], Loss: 0.5517\n",
      "Epoch [18/50], Step [310/6], Loss: 0.8372\n",
      "Epoch [18/50], Step [320/6], Loss: 0.6420\n",
      "Epoch [18/50], Step [330/6], Loss: 0.4927\n",
      "Epoch [18/50], Step [340/6], Loss: 0.5545\n",
      "Epoch [18/50], Step [350/6], Loss: 0.5600\n",
      "Epoch [18/50], Step [360/6], Loss: 0.5647\n",
      "Epoch [18/50], Step [370/6], Loss: 0.5890\n",
      "Epoch [19/50], Step [10/6], Loss: 0.7563\n",
      "Epoch [19/50], Step [20/6], Loss: 0.5360\n",
      "Epoch [19/50], Step [30/6], Loss: 0.5266\n",
      "Epoch [19/50], Step [40/6], Loss: 0.5409\n",
      "Epoch [19/50], Step [50/6], Loss: 0.5442\n",
      "Epoch [19/50], Step [60/6], Loss: 0.5904\n",
      "Epoch [19/50], Step [70/6], Loss: 0.8349\n",
      "Epoch [19/50], Step [80/6], Loss: 0.8587\n",
      "Epoch [19/50], Step [90/6], Loss: 0.5125\n",
      "Epoch [19/50], Step [100/6], Loss: 0.8890\n",
      "Epoch [19/50], Step [110/6], Loss: 0.5154\n",
      "Epoch [19/50], Step [120/6], Loss: 0.4083\n",
      "Epoch [19/50], Step [130/6], Loss: 0.8367\n",
      "Epoch [19/50], Step [140/6], Loss: 1.1015\n",
      "Epoch [19/50], Step [150/6], Loss: 0.5528\n",
      "Epoch [19/50], Step [160/6], Loss: 0.8927\n",
      "Epoch [19/50], Step [170/6], Loss: 0.5426\n",
      "Epoch [19/50], Step [180/6], Loss: 0.4342\n",
      "Epoch [19/50], Step [190/6], Loss: 0.6097\n",
      "Epoch [19/50], Step [200/6], Loss: 0.5374\n",
      "Epoch [19/50], Step [210/6], Loss: 0.8321\n",
      "Epoch [19/50], Step [220/6], Loss: 0.8504\n",
      "Epoch [19/50], Step [230/6], Loss: 0.5447\n",
      "Epoch [19/50], Step [240/6], Loss: 0.6310\n",
      "Epoch [19/50], Step [250/6], Loss: 0.5571\n",
      "Epoch [19/50], Step [260/6], Loss: 0.8315\n",
      "Epoch [19/50], Step [270/6], Loss: 0.5264\n",
      "Epoch [19/50], Step [280/6], Loss: 0.4584\n",
      "Epoch [19/50], Step [290/6], Loss: 0.7169\n",
      "Epoch [19/50], Step [300/6], Loss: 0.5465\n",
      "Epoch [19/50], Step [310/6], Loss: 0.8326\n",
      "Epoch [19/50], Step [320/6], Loss: 0.6505\n",
      "Epoch [19/50], Step [330/6], Loss: 0.4858\n",
      "Epoch [19/50], Step [340/6], Loss: 0.5504\n",
      "Epoch [19/50], Step [350/6], Loss: 0.5586\n",
      "Epoch [19/50], Step [360/6], Loss: 0.5668\n",
      "Epoch [19/50], Step [370/6], Loss: 0.5936\n",
      "Epoch [20/50], Step [10/6], Loss: 0.7466\n",
      "Epoch [20/50], Step [20/6], Loss: 0.5299\n",
      "Epoch [20/50], Step [30/6], Loss: 0.5210\n",
      "Epoch [20/50], Step [40/6], Loss: 0.5400\n",
      "Epoch [20/50], Step [50/6], Loss: 0.5409\n",
      "Epoch [20/50], Step [60/6], Loss: 0.5951\n",
      "Epoch [20/50], Step [70/6], Loss: 0.8224\n",
      "Epoch [20/50], Step [80/6], Loss: 0.8525\n",
      "Epoch [20/50], Step [90/6], Loss: 0.5118\n",
      "Epoch [20/50], Step [100/6], Loss: 0.8914\n",
      "Epoch [20/50], Step [110/6], Loss: 0.5156\n",
      "Epoch [20/50], Step [120/6], Loss: 0.3981\n",
      "Epoch [20/50], Step [130/6], Loss: 0.8327\n",
      "Epoch [20/50], Step [140/6], Loss: 1.1194\n",
      "Epoch [20/50], Step [150/6], Loss: 0.5551\n",
      "Epoch [20/50], Step [160/6], Loss: 0.8878\n",
      "Epoch [20/50], Step [170/6], Loss: 0.5429\n",
      "Epoch [20/50], Step [180/6], Loss: 0.4244\n",
      "Epoch [20/50], Step [190/6], Loss: 0.6175\n",
      "Epoch [20/50], Step [200/6], Loss: 0.5356\n",
      "Epoch [20/50], Step [210/6], Loss: 0.8237\n",
      "Epoch [20/50], Step [220/6], Loss: 0.8429\n",
      "Epoch [20/50], Step [230/6], Loss: 0.5410\n",
      "Epoch [20/50], Step [240/6], Loss: 0.6382\n",
      "Epoch [20/50], Step [250/6], Loss: 0.5554\n",
      "Epoch [20/50], Step [260/6], Loss: 0.8330\n",
      "Epoch [20/50], Step [270/6], Loss: 0.5223\n",
      "Epoch [20/50], Step [280/6], Loss: 0.4442\n",
      "Epoch [20/50], Step [290/6], Loss: 0.7059\n",
      "Epoch [20/50], Step [300/6], Loss: 0.5430\n",
      "Epoch [20/50], Step [310/6], Loss: 0.8205\n",
      "Epoch [20/50], Step [320/6], Loss: 0.6580\n",
      "Epoch [20/50], Step [330/6], Loss: 0.4807\n",
      "Epoch [20/50], Step [340/6], Loss: 0.5487\n",
      "Epoch [20/50], Step [350/6], Loss: 0.5585\n",
      "Epoch [20/50], Step [360/6], Loss: 0.5740\n",
      "Epoch [20/50], Step [370/6], Loss: 0.5993\n",
      "Epoch [21/50], Step [10/6], Loss: 0.7384\n",
      "Epoch [21/50], Step [20/6], Loss: 0.5259\n",
      "Epoch [21/50], Step [30/6], Loss: 0.5146\n",
      "Epoch [21/50], Step [40/6], Loss: 0.5272\n",
      "Epoch [21/50], Step [50/6], Loss: 0.5368\n",
      "Epoch [21/50], Step [60/6], Loss: 0.6009\n",
      "Epoch [21/50], Step [70/6], Loss: 0.8049\n",
      "Epoch [21/50], Step [80/6], Loss: 0.8597\n",
      "Epoch [21/50], Step [90/6], Loss: 0.5017\n",
      "Epoch [21/50], Step [100/6], Loss: 0.8956\n",
      "Epoch [21/50], Step [110/6], Loss: 0.5156\n",
      "Epoch [21/50], Step [120/6], Loss: 0.3863\n",
      "Epoch [21/50], Step [130/6], Loss: 0.8290\n",
      "Epoch [21/50], Step [140/6], Loss: 1.1396\n",
      "Epoch [21/50], Step [150/6], Loss: 0.5539\n",
      "Epoch [21/50], Step [160/6], Loss: 0.8784\n",
      "Epoch [21/50], Step [170/6], Loss: 0.5432\n",
      "Epoch [21/50], Step [180/6], Loss: 0.4137\n",
      "Epoch [21/50], Step [190/6], Loss: 0.6223\n",
      "Epoch [21/50], Step [200/6], Loss: 0.5350\n",
      "Epoch [21/50], Step [210/6], Loss: 0.8109\n",
      "Epoch [21/50], Step [220/6], Loss: 0.8320\n",
      "Epoch [21/50], Step [230/6], Loss: 0.5366\n",
      "Epoch [21/50], Step [240/6], Loss: 0.6483\n",
      "Epoch [21/50], Step [250/6], Loss: 0.5544\n",
      "Epoch [21/50], Step [260/6], Loss: 0.8332\n",
      "Epoch [21/50], Step [270/6], Loss: 0.5184\n",
      "Epoch [21/50], Step [280/6], Loss: 0.4326\n",
      "Epoch [21/50], Step [290/6], Loss: 0.6958\n",
      "Epoch [21/50], Step [300/6], Loss: 0.5372\n",
      "Epoch [21/50], Step [310/6], Loss: 0.8116\n",
      "Epoch [21/50], Step [320/6], Loss: 0.6658\n",
      "Epoch [21/50], Step [330/6], Loss: 0.4717\n",
      "Epoch [21/50], Step [340/6], Loss: 0.5447\n",
      "Epoch [21/50], Step [350/6], Loss: 0.5575\n",
      "Epoch [21/50], Step [360/6], Loss: 0.5854\n",
      "Epoch [21/50], Step [370/6], Loss: 0.6005\n",
      "Epoch [22/50], Step [10/6], Loss: 0.7312\n",
      "Epoch [22/50], Step [20/6], Loss: 0.5239\n",
      "Epoch [22/50], Step [30/6], Loss: 0.5189\n",
      "Epoch [22/50], Step [40/6], Loss: 0.5491\n",
      "Epoch [22/50], Step [50/6], Loss: 0.5286\n",
      "Epoch [22/50], Step [60/6], Loss: 0.5996\n",
      "Epoch [22/50], Step [70/6], Loss: 0.7967\n",
      "Epoch [22/50], Step [80/6], Loss: 0.8469\n",
      "Epoch [22/50], Step [90/6], Loss: 0.5029\n",
      "Epoch [22/50], Step [100/6], Loss: 0.9148\n",
      "Epoch [22/50], Step [110/6], Loss: 0.5110\n",
      "Epoch [22/50], Step [120/6], Loss: 0.3772\n",
      "Epoch [22/50], Step [130/6], Loss: 0.8322\n",
      "Epoch [22/50], Step [140/6], Loss: 1.1598\n",
      "Epoch [22/50], Step [150/6], Loss: 0.5528\n",
      "Epoch [22/50], Step [160/6], Loss: 0.8706\n",
      "Epoch [22/50], Step [170/6], Loss: 0.5401\n",
      "Epoch [22/50], Step [180/6], Loss: 0.4040\n",
      "Epoch [22/50], Step [190/6], Loss: 0.6252\n",
      "Epoch [22/50], Step [200/6], Loss: 0.5301\n",
      "Epoch [22/50], Step [210/6], Loss: 0.8019\n",
      "Epoch [22/50], Step [220/6], Loss: 0.8247\n",
      "Epoch [22/50], Step [230/6], Loss: 0.5292\n",
      "Epoch [22/50], Step [240/6], Loss: 0.6524\n",
      "Epoch [22/50], Step [250/6], Loss: 0.5503\n",
      "Epoch [22/50], Step [260/6], Loss: 0.8408\n",
      "Epoch [22/50], Step [270/6], Loss: 0.5112\n",
      "Epoch [22/50], Step [280/6], Loss: 0.4180\n",
      "Epoch [22/50], Step [290/6], Loss: 0.6872\n",
      "Epoch [22/50], Step [300/6], Loss: 0.5253\n",
      "Epoch [22/50], Step [310/6], Loss: 0.8028\n",
      "Epoch [22/50], Step [320/6], Loss: 0.6689\n",
      "Epoch [22/50], Step [330/6], Loss: 0.4656\n",
      "Epoch [22/50], Step [340/6], Loss: 0.5390\n",
      "Epoch [22/50], Step [350/6], Loss: 0.5525\n",
      "Epoch [22/50], Step [360/6], Loss: 0.5955\n",
      "Epoch [22/50], Step [370/6], Loss: 0.5975\n",
      "Epoch [23/50], Step [10/6], Loss: 0.7238\n",
      "Epoch [23/50], Step [20/6], Loss: 0.5211\n",
      "Epoch [23/50], Step [30/6], Loss: 0.5179\n",
      "Epoch [23/50], Step [40/6], Loss: 0.5494\n",
      "Epoch [23/50], Step [50/6], Loss: 0.5241\n",
      "Epoch [23/50], Step [60/6], Loss: 0.6014\n",
      "Epoch [23/50], Step [70/6], Loss: 0.7784\n",
      "Epoch [23/50], Step [80/6], Loss: 0.8331\n",
      "Epoch [23/50], Step [90/6], Loss: 0.5038\n",
      "Epoch [23/50], Step [100/6], Loss: 0.9256\n",
      "Epoch [23/50], Step [110/6], Loss: 0.5084\n",
      "Epoch [23/50], Step [120/6], Loss: 0.3672\n",
      "Epoch [23/50], Step [130/6], Loss: 0.8327\n",
      "Epoch [23/50], Step [140/6], Loss: 1.1793\n",
      "Epoch [23/50], Step [150/6], Loss: 0.5530\n",
      "Epoch [23/50], Step [160/6], Loss: 0.8653\n",
      "Epoch [23/50], Step [170/6], Loss: 0.5354\n",
      "Epoch [23/50], Step [180/6], Loss: 0.3930\n",
      "Epoch [23/50], Step [190/6], Loss: 0.6299\n",
      "Epoch [23/50], Step [200/6], Loss: 0.5279\n",
      "Epoch [23/50], Step [210/6], Loss: 0.7896\n",
      "Epoch [23/50], Step [220/6], Loss: 0.8125\n",
      "Epoch [23/50], Step [230/6], Loss: 0.5270\n",
      "Epoch [23/50], Step [240/6], Loss: 0.6601\n",
      "Epoch [23/50], Step [250/6], Loss: 0.5461\n",
      "Epoch [23/50], Step [260/6], Loss: 0.8429\n",
      "Epoch [23/50], Step [270/6], Loss: 0.5067\n",
      "Epoch [23/50], Step [280/6], Loss: 0.3994\n",
      "Epoch [23/50], Step [290/6], Loss: 0.6729\n",
      "Epoch [23/50], Step [300/6], Loss: 0.5206\n",
      "Epoch [23/50], Step [310/6], Loss: 0.7923\n",
      "Epoch [23/50], Step [320/6], Loss: 0.6787\n",
      "Epoch [23/50], Step [330/6], Loss: 0.4569\n",
      "Epoch [23/50], Step [340/6], Loss: 0.5344\n",
      "Epoch [23/50], Step [350/6], Loss: 0.5538\n",
      "Epoch [23/50], Step [360/6], Loss: 0.6064\n",
      "Epoch [23/50], Step [370/6], Loss: 0.6032\n",
      "Epoch [24/50], Step [10/6], Loss: 0.7110\n",
      "Epoch [24/50], Step [20/6], Loss: 0.5164\n",
      "Epoch [24/50], Step [30/6], Loss: 0.5139\n",
      "Epoch [24/50], Step [40/6], Loss: 0.5395\n",
      "Epoch [24/50], Step [50/6], Loss: 0.5182\n",
      "Epoch [24/50], Step [60/6], Loss: 0.6063\n",
      "Epoch [24/50], Step [70/6], Loss: 0.7649\n",
      "Epoch [24/50], Step [80/6], Loss: 0.8543\n",
      "Epoch [24/50], Step [90/6], Loss: 0.4832\n",
      "Epoch [24/50], Step [100/6], Loss: 0.9367\n",
      "Epoch [24/50], Step [110/6], Loss: 0.5058\n",
      "Epoch [24/50], Step [120/6], Loss: 0.3510\n",
      "Epoch [24/50], Step [130/6], Loss: 0.8316\n",
      "Epoch [24/50], Step [140/6], Loss: 1.2090\n",
      "Epoch [24/50], Step [150/6], Loss: 0.5510\n",
      "Epoch [24/50], Step [160/6], Loss: 0.8625\n",
      "Epoch [24/50], Step [170/6], Loss: 0.5315\n",
      "Epoch [24/50], Step [180/6], Loss: 0.3788\n",
      "Epoch [24/50], Step [190/6], Loss: 0.6366\n",
      "Epoch [24/50], Step [200/6], Loss: 0.5237\n",
      "Epoch [24/50], Step [210/6], Loss: 0.7794\n",
      "Epoch [24/50], Step [220/6], Loss: 0.8035\n",
      "Epoch [24/50], Step [230/6], Loss: 0.5233\n",
      "Epoch [24/50], Step [240/6], Loss: 0.6680\n",
      "Epoch [24/50], Step [250/6], Loss: 0.5417\n",
      "Epoch [24/50], Step [260/6], Loss: 0.8450\n",
      "Epoch [24/50], Step [270/6], Loss: 0.5002\n",
      "Epoch [24/50], Step [280/6], Loss: 0.3856\n",
      "Epoch [24/50], Step [290/6], Loss: 0.6589\n",
      "Epoch [24/50], Step [300/6], Loss: 0.5180\n",
      "Epoch [24/50], Step [310/6], Loss: 0.7862\n",
      "Epoch [24/50], Step [320/6], Loss: 0.6875\n",
      "Epoch [24/50], Step [330/6], Loss: 0.4459\n",
      "Epoch [24/50], Step [340/6], Loss: 0.5271\n",
      "Epoch [24/50], Step [350/6], Loss: 0.5503\n",
      "Epoch [24/50], Step [360/6], Loss: 0.6079\n",
      "Epoch [24/50], Step [370/6], Loss: 0.6028\n",
      "Epoch [25/50], Step [10/6], Loss: 0.6995\n",
      "Epoch [25/50], Step [20/6], Loss: 0.5073\n",
      "Epoch [25/50], Step [30/6], Loss: 0.5022\n",
      "Epoch [25/50], Step [40/6], Loss: 0.5537\n",
      "Epoch [25/50], Step [50/6], Loss: 0.5119\n",
      "Epoch [25/50], Step [60/6], Loss: 0.6097\n",
      "Epoch [25/50], Step [70/6], Loss: 0.7479\n",
      "Epoch [25/50], Step [80/6], Loss: 0.8330\n",
      "Epoch [25/50], Step [90/6], Loss: 0.4886\n",
      "Epoch [25/50], Step [100/6], Loss: 0.9453\n",
      "Epoch [25/50], Step [110/6], Loss: 0.5041\n",
      "Epoch [25/50], Step [120/6], Loss: 0.3428\n",
      "Epoch [25/50], Step [130/6], Loss: 0.8268\n",
      "Epoch [25/50], Step [140/6], Loss: 1.2278\n",
      "Epoch [25/50], Step [150/6], Loss: 0.5492\n",
      "Epoch [25/50], Step [160/6], Loss: 0.8535\n",
      "Epoch [25/50], Step [170/6], Loss: 0.5334\n",
      "Epoch [25/50], Step [180/6], Loss: 0.3685\n",
      "Epoch [25/50], Step [190/6], Loss: 0.6391\n",
      "Epoch [25/50], Step [200/6], Loss: 0.5261\n",
      "Epoch [25/50], Step [210/6], Loss: 0.7654\n",
      "Epoch [25/50], Step [220/6], Loss: 0.7911\n",
      "Epoch [25/50], Step [230/6], Loss: 0.5128\n",
      "Epoch [25/50], Step [240/6], Loss: 0.6710\n",
      "Epoch [25/50], Step [250/6], Loss: 0.5354\n",
      "Epoch [25/50], Step [260/6], Loss: 0.8486\n",
      "Epoch [25/50], Step [270/6], Loss: 0.4954\n",
      "Epoch [25/50], Step [280/6], Loss: 0.3695\n",
      "Epoch [25/50], Step [290/6], Loss: 0.6453\n",
      "Epoch [25/50], Step [300/6], Loss: 0.5109\n",
      "Epoch [25/50], Step [310/6], Loss: 0.7766\n",
      "Epoch [25/50], Step [320/6], Loss: 0.6964\n",
      "Epoch [25/50], Step [330/6], Loss: 0.4366\n",
      "Epoch [25/50], Step [340/6], Loss: 0.5214\n",
      "Epoch [25/50], Step [350/6], Loss: 0.5505\n",
      "Epoch [25/50], Step [360/6], Loss: 0.6266\n",
      "Epoch [25/50], Step [370/6], Loss: 0.6067\n",
      "Epoch [26/50], Step [10/6], Loss: 0.6892\n",
      "Epoch [26/50], Step [20/6], Loss: 0.5084\n",
      "Epoch [26/50], Step [30/6], Loss: 0.5073\n",
      "Epoch [26/50], Step [40/6], Loss: 0.5418\n",
      "Epoch [26/50], Step [50/6], Loss: 0.5069\n",
      "Epoch [26/50], Step [60/6], Loss: 0.6120\n",
      "Epoch [26/50], Step [70/6], Loss: 0.7357\n",
      "Epoch [26/50], Step [80/6], Loss: 0.8418\n",
      "Epoch [26/50], Step [90/6], Loss: 0.4742\n",
      "Epoch [26/50], Step [100/6], Loss: 0.9492\n",
      "Epoch [26/50], Step [110/6], Loss: 0.5056\n",
      "Epoch [26/50], Step [120/6], Loss: 0.3283\n",
      "Epoch [26/50], Step [130/6], Loss: 0.8225\n",
      "Epoch [26/50], Step [140/6], Loss: 1.2477\n",
      "Epoch [26/50], Step [150/6], Loss: 0.5515\n",
      "Epoch [26/50], Step [160/6], Loss: 0.8390\n",
      "Epoch [26/50], Step [170/6], Loss: 0.5293\n",
      "Epoch [26/50], Step [180/6], Loss: 0.3574\n",
      "Epoch [26/50], Step [190/6], Loss: 0.6472\n",
      "Epoch [26/50], Step [200/6], Loss: 0.5169\n",
      "Epoch [26/50], Step [210/6], Loss: 0.7538\n",
      "Epoch [26/50], Step [220/6], Loss: 0.7764\n",
      "Epoch [26/50], Step [230/6], Loss: 0.5131\n",
      "Epoch [26/50], Step [240/6], Loss: 0.6819\n",
      "Epoch [26/50], Step [250/6], Loss: 0.5302\n",
      "Epoch [26/50], Step [260/6], Loss: 0.8498\n",
      "Epoch [26/50], Step [270/6], Loss: 0.4931\n",
      "Epoch [26/50], Step [280/6], Loss: 0.3628\n",
      "Epoch [26/50], Step [290/6], Loss: 0.6332\n",
      "Epoch [26/50], Step [300/6], Loss: 0.5052\n",
      "Epoch [26/50], Step [310/6], Loss: 0.7674\n",
      "Epoch [26/50], Step [320/6], Loss: 0.7065\n",
      "Epoch [26/50], Step [330/6], Loss: 0.4293\n",
      "Epoch [26/50], Step [340/6], Loss: 0.5156\n",
      "Epoch [26/50], Step [350/6], Loss: 0.5543\n",
      "Epoch [26/50], Step [360/6], Loss: 0.6374\n",
      "Epoch [26/50], Step [370/6], Loss: 0.6133\n",
      "Epoch [27/50], Step [10/6], Loss: 0.6751\n",
      "Epoch [27/50], Step [20/6], Loss: 0.5032\n",
      "Epoch [27/50], Step [30/6], Loss: 0.5057\n",
      "Epoch [27/50], Step [40/6], Loss: 0.5448\n",
      "Epoch [27/50], Step [50/6], Loss: 0.5068\n",
      "Epoch [27/50], Step [60/6], Loss: 0.6211\n",
      "Epoch [27/50], Step [70/6], Loss: 0.7123\n",
      "Epoch [27/50], Step [80/6], Loss: 0.8399\n",
      "Epoch [27/50], Step [90/6], Loss: 0.4681\n",
      "Epoch [27/50], Step [100/6], Loss: 0.9472\n",
      "Epoch [27/50], Step [110/6], Loss: 0.5060\n",
      "Epoch [27/50], Step [120/6], Loss: 0.3179\n",
      "Epoch [27/50], Step [130/6], Loss: 0.8121\n",
      "Epoch [27/50], Step [140/6], Loss: 1.2687\n",
      "Epoch [27/50], Step [150/6], Loss: 0.5525\n",
      "Epoch [27/50], Step [160/6], Loss: 0.8238\n",
      "Epoch [27/50], Step [170/6], Loss: 0.5349\n",
      "Epoch [27/50], Step [180/6], Loss: 0.3473\n",
      "Epoch [27/50], Step [190/6], Loss: 0.6517\n",
      "Epoch [27/50], Step [200/6], Loss: 0.5205\n",
      "Epoch [27/50], Step [210/6], Loss: 0.7413\n",
      "Epoch [27/50], Step [220/6], Loss: 0.7641\n",
      "Epoch [27/50], Step [230/6], Loss: 0.5076\n",
      "Epoch [27/50], Step [240/6], Loss: 0.6861\n",
      "Epoch [27/50], Step [250/6], Loss: 0.5234\n",
      "Epoch [27/50], Step [260/6], Loss: 0.8560\n",
      "Epoch [27/50], Step [270/6], Loss: 0.4858\n",
      "Epoch [27/50], Step [280/6], Loss: 0.3557\n",
      "Epoch [27/50], Step [290/6], Loss: 0.6218\n",
      "Epoch [27/50], Step [300/6], Loss: 0.4973\n",
      "Epoch [27/50], Step [310/6], Loss: 0.7577\n",
      "Epoch [27/50], Step [320/6], Loss: 0.7149\n",
      "Epoch [27/50], Step [330/6], Loss: 0.4223\n",
      "Epoch [27/50], Step [340/6], Loss: 0.5087\n",
      "Epoch [27/50], Step [350/6], Loss: 0.5520\n",
      "Epoch [27/50], Step [360/6], Loss: 0.6412\n",
      "Epoch [27/50], Step [370/6], Loss: 0.6147\n",
      "Epoch [28/50], Step [10/6], Loss: 0.6643\n",
      "Epoch [28/50], Step [20/6], Loss: 0.4980\n",
      "Epoch [28/50], Step [30/6], Loss: 0.4973\n",
      "Epoch [28/50], Step [40/6], Loss: 0.5719\n",
      "Epoch [28/50], Step [50/6], Loss: 0.4978\n",
      "Epoch [28/50], Step [60/6], Loss: 0.6248\n",
      "Epoch [28/50], Step [70/6], Loss: 0.7030\n",
      "Epoch [28/50], Step [80/6], Loss: 0.8149\n",
      "Epoch [28/50], Step [90/6], Loss: 0.4714\n",
      "Epoch [28/50], Step [100/6], Loss: 0.9563\n",
      "Epoch [28/50], Step [110/6], Loss: 0.5034\n",
      "Epoch [28/50], Step [120/6], Loss: 0.3107\n",
      "Epoch [28/50], Step [130/6], Loss: 0.8045\n",
      "Epoch [28/50], Step [140/6], Loss: 1.2893\n",
      "Epoch [28/50], Step [150/6], Loss: 0.5519\n",
      "Epoch [28/50], Step [160/6], Loss: 0.8133\n",
      "Epoch [28/50], Step [170/6], Loss: 0.5307\n",
      "Epoch [28/50], Step [180/6], Loss: 0.3377\n",
      "Epoch [28/50], Step [190/6], Loss: 0.6566\n",
      "Epoch [28/50], Step [200/6], Loss: 0.5171\n",
      "Epoch [28/50], Step [210/6], Loss: 0.7346\n",
      "Epoch [28/50], Step [220/6], Loss: 0.7605\n",
      "Epoch [28/50], Step [230/6], Loss: 0.5006\n",
      "Epoch [28/50], Step [240/6], Loss: 0.6970\n",
      "Epoch [28/50], Step [250/6], Loss: 0.5206\n",
      "Epoch [28/50], Step [260/6], Loss: 0.8556\n",
      "Epoch [28/50], Step [270/6], Loss: 0.4788\n",
      "Epoch [28/50], Step [280/6], Loss: 0.3408\n",
      "Epoch [28/50], Step [290/6], Loss: 0.6102\n",
      "Epoch [28/50], Step [300/6], Loss: 0.4954\n",
      "Epoch [28/50], Step [310/6], Loss: 0.7480\n",
      "Epoch [28/50], Step [320/6], Loss: 0.7220\n",
      "Epoch [28/50], Step [330/6], Loss: 0.4117\n",
      "Epoch [28/50], Step [340/6], Loss: 0.5014\n",
      "Epoch [28/50], Step [350/6], Loss: 0.5544\n",
      "Epoch [28/50], Step [360/6], Loss: 0.6518\n",
      "Epoch [28/50], Step [370/6], Loss: 0.6185\n",
      "Epoch [29/50], Step [10/6], Loss: 0.6537\n",
      "Epoch [29/50], Step [20/6], Loss: 0.4947\n",
      "Epoch [29/50], Step [30/6], Loss: 0.4974\n",
      "Epoch [29/50], Step [40/6], Loss: 0.5459\n",
      "Epoch [29/50], Step [50/6], Loss: 0.4990\n",
      "Epoch [29/50], Step [60/6], Loss: 0.6295\n",
      "Epoch [29/50], Step [70/6], Loss: 0.6886\n",
      "Epoch [29/50], Step [80/6], Loss: 0.8384\n",
      "Epoch [29/50], Step [90/6], Loss: 0.4521\n",
      "Epoch [29/50], Step [100/6], Loss: 0.9540\n",
      "Epoch [29/50], Step [110/6], Loss: 0.5031\n",
      "Epoch [29/50], Step [120/6], Loss: 0.2991\n",
      "Epoch [29/50], Step [130/6], Loss: 0.7928\n",
      "Epoch [29/50], Step [140/6], Loss: 1.2968\n",
      "Epoch [29/50], Step [150/6], Loss: 0.5533\n",
      "Epoch [29/50], Step [160/6], Loss: 0.8017\n",
      "Epoch [29/50], Step [170/6], Loss: 0.5323\n",
      "Epoch [29/50], Step [180/6], Loss: 0.3303\n",
      "Epoch [29/50], Step [190/6], Loss: 0.6617\n",
      "Epoch [29/50], Step [200/6], Loss: 0.5170\n",
      "Epoch [29/50], Step [210/6], Loss: 0.7238\n",
      "Epoch [29/50], Step [220/6], Loss: 0.7484\n",
      "Epoch [29/50], Step [230/6], Loss: 0.4981\n",
      "Epoch [29/50], Step [240/6], Loss: 0.7074\n",
      "Epoch [29/50], Step [250/6], Loss: 0.5149\n",
      "Epoch [29/50], Step [260/6], Loss: 0.8614\n",
      "Epoch [29/50], Step [270/6], Loss: 0.4734\n",
      "Epoch [29/50], Step [280/6], Loss: 0.3438\n",
      "Epoch [29/50], Step [290/6], Loss: 0.6005\n",
      "Epoch [29/50], Step [300/6], Loss: 0.4900\n",
      "Epoch [29/50], Step [310/6], Loss: 0.7399\n",
      "Epoch [29/50], Step [320/6], Loss: 0.7291\n",
      "Epoch [29/50], Step [330/6], Loss: 0.4059\n",
      "Epoch [29/50], Step [340/6], Loss: 0.4930\n",
      "Epoch [29/50], Step [350/6], Loss: 0.5544\n",
      "Epoch [29/50], Step [360/6], Loss: 0.6587\n",
      "Epoch [29/50], Step [370/6], Loss: 0.6216\n",
      "Epoch [30/50], Step [10/6], Loss: 0.6425\n",
      "Epoch [30/50], Step [20/6], Loss: 0.4897\n",
      "Epoch [30/50], Step [30/6], Loss: 0.4859\n",
      "Epoch [30/50], Step [40/6], Loss: 0.5451\n",
      "Epoch [30/50], Step [50/6], Loss: 0.4927\n",
      "Epoch [30/50], Step [60/6], Loss: 0.6344\n",
      "Epoch [30/50], Step [70/6], Loss: 0.6755\n",
      "Epoch [30/50], Step [80/6], Loss: 0.8442\n",
      "Epoch [30/50], Step [90/6], Loss: 0.4384\n",
      "Epoch [30/50], Step [100/6], Loss: 0.9602\n",
      "Epoch [30/50], Step [110/6], Loss: 0.5025\n",
      "Epoch [30/50], Step [120/6], Loss: 0.2893\n",
      "Epoch [30/50], Step [130/6], Loss: 0.7822\n",
      "Epoch [30/50], Step [140/6], Loss: 1.3235\n",
      "Epoch [30/50], Step [150/6], Loss: 0.5488\n",
      "Epoch [30/50], Step [160/6], Loss: 0.7925\n",
      "Epoch [30/50], Step [170/6], Loss: 0.5279\n",
      "Epoch [30/50], Step [180/6], Loss: 0.3203\n",
      "Epoch [30/50], Step [190/6], Loss: 0.6632\n",
      "Epoch [30/50], Step [200/6], Loss: 0.5154\n",
      "Epoch [30/50], Step [210/6], Loss: 0.7131\n",
      "Epoch [30/50], Step [220/6], Loss: 0.7388\n",
      "Epoch [30/50], Step [230/6], Loss: 0.4911\n",
      "Epoch [30/50], Step [240/6], Loss: 0.7150\n",
      "Epoch [30/50], Step [250/6], Loss: 0.5077\n",
      "Epoch [30/50], Step [260/6], Loss: 0.8696\n",
      "Epoch [30/50], Step [270/6], Loss: 0.4702\n",
      "Epoch [30/50], Step [280/6], Loss: 0.3359\n",
      "Epoch [30/50], Step [290/6], Loss: 0.5929\n",
      "Epoch [30/50], Step [300/6], Loss: 0.4814\n",
      "Epoch [30/50], Step [310/6], Loss: 0.7354\n",
      "Epoch [30/50], Step [320/6], Loss: 0.7347\n",
      "Epoch [30/50], Step [330/6], Loss: 0.4008\n",
      "Epoch [30/50], Step [340/6], Loss: 0.4835\n",
      "Epoch [30/50], Step [350/6], Loss: 0.5560\n",
      "Epoch [30/50], Step [360/6], Loss: 0.6678\n",
      "Epoch [30/50], Step [370/6], Loss: 0.6206\n",
      "Epoch [31/50], Step [10/6], Loss: 0.6313\n",
      "Epoch [31/50], Step [20/6], Loss: 0.4859\n",
      "Epoch [31/50], Step [30/6], Loss: 0.4851\n",
      "Epoch [31/50], Step [40/6], Loss: 0.5509\n",
      "Epoch [31/50], Step [50/6], Loss: 0.4868\n",
      "Epoch [31/50], Step [60/6], Loss: 0.6353\n",
      "Epoch [31/50], Step [70/6], Loss: 0.6637\n",
      "Epoch [31/50], Step [80/6], Loss: 0.8430\n",
      "Epoch [31/50], Step [90/6], Loss: 0.4317\n",
      "Epoch [31/50], Step [100/6], Loss: 0.9686\n",
      "Epoch [31/50], Step [110/6], Loss: 0.5086\n",
      "Epoch [31/50], Step [120/6], Loss: 0.2835\n",
      "Epoch [31/50], Step [130/6], Loss: 0.7729\n",
      "Epoch [31/50], Step [140/6], Loss: 1.3196\n",
      "Epoch [31/50], Step [150/6], Loss: 0.5495\n",
      "Epoch [31/50], Step [160/6], Loss: 0.7752\n",
      "Epoch [31/50], Step [170/6], Loss: 0.5250\n",
      "Epoch [31/50], Step [180/6], Loss: 0.3107\n",
      "Epoch [31/50], Step [190/6], Loss: 0.6693\n",
      "Epoch [31/50], Step [200/6], Loss: 0.5110\n",
      "Epoch [31/50], Step [210/6], Loss: 0.7037\n",
      "Epoch [31/50], Step [220/6], Loss: 0.7262\n",
      "Epoch [31/50], Step [230/6], Loss: 0.4874\n",
      "Epoch [31/50], Step [240/6], Loss: 0.7254\n",
      "Epoch [31/50], Step [250/6], Loss: 0.5046\n",
      "Epoch [31/50], Step [260/6], Loss: 0.8725\n",
      "Epoch [31/50], Step [270/6], Loss: 0.4640\n",
      "Epoch [31/50], Step [280/6], Loss: 0.3469\n",
      "Epoch [31/50], Step [290/6], Loss: 0.5827\n",
      "Epoch [31/50], Step [300/6], Loss: 0.4751\n",
      "Epoch [31/50], Step [310/6], Loss: 0.7194\n",
      "Epoch [31/50], Step [320/6], Loss: 0.7396\n",
      "Epoch [31/50], Step [330/6], Loss: 0.4049\n",
      "Epoch [31/50], Step [340/6], Loss: 0.4782\n",
      "Epoch [31/50], Step [350/6], Loss: 0.5605\n",
      "Epoch [31/50], Step [360/6], Loss: 0.6703\n",
      "Epoch [31/50], Step [370/6], Loss: 0.6230\n",
      "Epoch [32/50], Step [10/6], Loss: 0.6114\n",
      "Epoch [32/50], Step [20/6], Loss: 0.4766\n",
      "Epoch [32/50], Step [30/6], Loss: 0.4718\n",
      "Epoch [32/50], Step [40/6], Loss: 0.5553\n",
      "Epoch [32/50], Step [50/6], Loss: 0.4808\n",
      "Epoch [32/50], Step [60/6], Loss: 0.6397\n",
      "Epoch [32/50], Step [70/6], Loss: 0.6581\n",
      "Epoch [32/50], Step [80/6], Loss: 0.8376\n",
      "Epoch [32/50], Step [90/6], Loss: 0.4246\n",
      "Epoch [32/50], Step [100/6], Loss: 0.9757\n",
      "Epoch [32/50], Step [110/6], Loss: 0.5030\n",
      "Epoch [32/50], Step [120/6], Loss: 0.2773\n",
      "Epoch [32/50], Step [130/6], Loss: 0.7573\n",
      "Epoch [32/50], Step [140/6], Loss: 1.3331\n",
      "Epoch [32/50], Step [150/6], Loss: 0.5460\n",
      "Epoch [32/50], Step [160/6], Loss: 0.7678\n",
      "Epoch [32/50], Step [170/6], Loss: 0.5239\n",
      "Epoch [32/50], Step [180/6], Loss: 0.3017\n",
      "Epoch [32/50], Step [190/6], Loss: 0.6725\n",
      "Epoch [32/50], Step [200/6], Loss: 0.5071\n",
      "Epoch [32/50], Step [210/6], Loss: 0.6967\n",
      "Epoch [32/50], Step [220/6], Loss: 0.7211\n",
      "Epoch [32/50], Step [230/6], Loss: 0.4795\n",
      "Epoch [32/50], Step [240/6], Loss: 0.7329\n",
      "Epoch [32/50], Step [250/6], Loss: 0.4962\n",
      "Epoch [32/50], Step [260/6], Loss: 0.8809\n",
      "Epoch [32/50], Step [270/6], Loss: 0.4581\n",
      "Epoch [32/50], Step [280/6], Loss: 0.3511\n",
      "Epoch [32/50], Step [290/6], Loss: 0.5750\n",
      "Epoch [32/50], Step [300/6], Loss: 0.4702\n",
      "Epoch [32/50], Step [310/6], Loss: 0.7173\n",
      "Epoch [32/50], Step [320/6], Loss: 0.7504\n",
      "Epoch [32/50], Step [330/6], Loss: 0.4003\n",
      "Epoch [32/50], Step [340/6], Loss: 0.4675\n",
      "Epoch [32/50], Step [350/6], Loss: 0.5644\n",
      "Epoch [32/50], Step [360/6], Loss: 0.6763\n",
      "Epoch [32/50], Step [370/6], Loss: 0.6252\n",
      "Epoch [33/50], Step [10/6], Loss: 0.5977\n",
      "Epoch [33/50], Step [20/6], Loss: 0.4725\n",
      "Epoch [33/50], Step [30/6], Loss: 0.4647\n",
      "Epoch [33/50], Step [40/6], Loss: 0.5554\n",
      "Epoch [33/50], Step [50/6], Loss: 0.4747\n",
      "Epoch [33/50], Step [60/6], Loss: 0.6438\n",
      "Epoch [33/50], Step [70/6], Loss: 0.6493\n",
      "Epoch [33/50], Step [80/6], Loss: 0.8394\n",
      "Epoch [33/50], Step [90/6], Loss: 0.4157\n",
      "Epoch [33/50], Step [100/6], Loss: 0.9802\n",
      "Epoch [33/50], Step [110/6], Loss: 0.5017\n",
      "Epoch [33/50], Step [120/6], Loss: 0.2683\n",
      "Epoch [33/50], Step [130/6], Loss: 0.7483\n",
      "Epoch [33/50], Step [140/6], Loss: 1.3419\n",
      "Epoch [33/50], Step [150/6], Loss: 0.5394\n",
      "Epoch [33/50], Step [160/6], Loss: 0.7594\n",
      "Epoch [33/50], Step [170/6], Loss: 0.5259\n",
      "Epoch [33/50], Step [180/6], Loss: 0.2951\n",
      "Epoch [33/50], Step [190/6], Loss: 0.6716\n",
      "Epoch [33/50], Step [200/6], Loss: 0.5093\n",
      "Epoch [33/50], Step [210/6], Loss: 0.6909\n",
      "Epoch [33/50], Step [220/6], Loss: 0.7140\n",
      "Epoch [33/50], Step [230/6], Loss: 0.4718\n",
      "Epoch [33/50], Step [240/6], Loss: 0.7404\n",
      "Epoch [33/50], Step [250/6], Loss: 0.4890\n",
      "Epoch [33/50], Step [260/6], Loss: 0.8917\n",
      "Epoch [33/50], Step [270/6], Loss: 0.4510\n",
      "Epoch [33/50], Step [280/6], Loss: 0.3490\n",
      "Epoch [33/50], Step [290/6], Loss: 0.5680\n",
      "Epoch [33/50], Step [300/6], Loss: 0.4613\n",
      "Epoch [33/50], Step [310/6], Loss: 0.7184\n",
      "Epoch [33/50], Step [320/6], Loss: 0.7572\n",
      "Epoch [33/50], Step [330/6], Loss: 0.3970\n",
      "Epoch [33/50], Step [340/6], Loss: 0.4572\n",
      "Epoch [33/50], Step [350/6], Loss: 0.5659\n",
      "Epoch [33/50], Step [360/6], Loss: 0.6874\n",
      "Epoch [33/50], Step [370/6], Loss: 0.6266\n",
      "Epoch [34/50], Step [10/6], Loss: 0.5860\n",
      "Epoch [34/50], Step [20/6], Loss: 0.4704\n",
      "Epoch [34/50], Step [30/6], Loss: 0.4544\n",
      "Epoch [34/50], Step [40/6], Loss: 0.5949\n",
      "Epoch [34/50], Step [50/6], Loss: 0.4598\n",
      "Epoch [34/50], Step [60/6], Loss: 0.6399\n",
      "Epoch [34/50], Step [70/6], Loss: 0.6433\n",
      "Epoch [34/50], Step [80/6], Loss: 0.7969\n",
      "Epoch [34/50], Step [90/6], Loss: 0.4349\n",
      "Epoch [34/50], Step [100/6], Loss: 1.0076\n",
      "Epoch [34/50], Step [110/6], Loss: 0.4987\n",
      "Epoch [34/50], Step [120/6], Loss: 0.2740\n",
      "Epoch [34/50], Step [130/6], Loss: 0.7476\n",
      "Epoch [34/50], Step [140/6], Loss: 1.3425\n",
      "Epoch [34/50], Step [150/6], Loss: 0.5314\n",
      "Epoch [34/50], Step [160/6], Loss: 0.7536\n",
      "Epoch [34/50], Step [170/6], Loss: 0.5192\n",
      "Epoch [34/50], Step [180/6], Loss: 0.2908\n",
      "Epoch [34/50], Step [190/6], Loss: 0.6716\n",
      "Epoch [34/50], Step [200/6], Loss: 0.5058\n",
      "Epoch [34/50], Step [210/6], Loss: 0.6801\n",
      "Epoch [34/50], Step [220/6], Loss: 0.7062\n",
      "Epoch [34/50], Step [230/6], Loss: 0.4698\n",
      "Epoch [34/50], Step [240/6], Loss: 0.7569\n",
      "Epoch [34/50], Step [250/6], Loss: 0.4877\n",
      "Epoch [34/50], Step [260/6], Loss: 0.8894\n",
      "Epoch [34/50], Step [270/6], Loss: 0.4497\n",
      "Epoch [34/50], Step [280/6], Loss: 0.3373\n",
      "Epoch [34/50], Step [290/6], Loss: 0.5594\n",
      "Epoch [34/50], Step [300/6], Loss: 0.4567\n",
      "Epoch [34/50], Step [310/6], Loss: 0.7060\n",
      "Epoch [34/50], Step [320/6], Loss: 0.7617\n",
      "Epoch [34/50], Step [330/6], Loss: 0.3848\n",
      "Epoch [34/50], Step [340/6], Loss: 0.4468\n",
      "Epoch [34/50], Step [350/6], Loss: 0.5708\n",
      "Epoch [34/50], Step [360/6], Loss: 0.6960\n",
      "Epoch [34/50], Step [370/6], Loss: 0.6334\n",
      "Epoch [35/50], Step [10/6], Loss: 0.5748\n",
      "Epoch [35/50], Step [20/6], Loss: 0.4678\n",
      "Epoch [35/50], Step [30/6], Loss: 0.4487\n",
      "Epoch [35/50], Step [40/6], Loss: 0.5703\n",
      "Epoch [35/50], Step [50/6], Loss: 0.4585\n",
      "Epoch [35/50], Step [60/6], Loss: 0.6462\n",
      "Epoch [35/50], Step [70/6], Loss: 0.6222\n",
      "Epoch [35/50], Step [80/6], Loss: 0.8428\n",
      "Epoch [35/50], Step [90/6], Loss: 0.4039\n",
      "Epoch [35/50], Step [100/6], Loss: 0.9975\n",
      "Epoch [35/50], Step [110/6], Loss: 0.5001\n",
      "Epoch [35/50], Step [120/6], Loss: 0.2584\n",
      "Epoch [35/50], Step [130/6], Loss: 0.7346\n",
      "Epoch [35/50], Step [140/6], Loss: 1.3528\n",
      "Epoch [35/50], Step [150/6], Loss: 0.5270\n",
      "Epoch [35/50], Step [160/6], Loss: 0.7317\n",
      "Epoch [35/50], Step [170/6], Loss: 0.5206\n",
      "Epoch [35/50], Step [180/6], Loss: 0.2818\n",
      "Epoch [35/50], Step [190/6], Loss: 0.6697\n",
      "Epoch [35/50], Step [200/6], Loss: 0.5076\n",
      "Epoch [35/50], Step [210/6], Loss: 0.6633\n",
      "Epoch [35/50], Step [220/6], Loss: 0.6926\n",
      "Epoch [35/50], Step [230/6], Loss: 0.4610\n",
      "Epoch [35/50], Step [240/6], Loss: 0.7577\n",
      "Epoch [35/50], Step [250/6], Loss: 0.4848\n",
      "Epoch [35/50], Step [260/6], Loss: 0.8947\n",
      "Epoch [35/50], Step [270/6], Loss: 0.4463\n",
      "Epoch [35/50], Step [280/6], Loss: 0.3356\n",
      "Epoch [35/50], Step [290/6], Loss: 0.5513\n",
      "Epoch [35/50], Step [300/6], Loss: 0.4562\n",
      "Epoch [35/50], Step [310/6], Loss: 0.6965\n",
      "Epoch [35/50], Step [320/6], Loss: 0.7560\n",
      "Epoch [35/50], Step [330/6], Loss: 0.3808\n",
      "Epoch [35/50], Step [340/6], Loss: 0.4354\n",
      "Epoch [35/50], Step [350/6], Loss: 0.5662\n",
      "Epoch [35/50], Step [360/6], Loss: 0.6957\n",
      "Epoch [35/50], Step [370/6], Loss: 0.6302\n",
      "Epoch [36/50], Step [10/6], Loss: 0.5689\n",
      "Epoch [36/50], Step [20/6], Loss: 0.4596\n",
      "Epoch [36/50], Step [30/6], Loss: 0.4333\n",
      "Epoch [36/50], Step [40/6], Loss: 0.5735\n",
      "Epoch [36/50], Step [50/6], Loss: 0.4566\n",
      "Epoch [36/50], Step [60/6], Loss: 0.6480\n",
      "Epoch [36/50], Step [70/6], Loss: 0.5993\n",
      "Epoch [36/50], Step [80/6], Loss: 0.8348\n",
      "Epoch [36/50], Step [90/6], Loss: 0.3993\n",
      "Epoch [36/50], Step [100/6], Loss: 0.9910\n",
      "Epoch [36/50], Step [110/6], Loss: 0.5053\n",
      "Epoch [36/50], Step [120/6], Loss: 0.2513\n",
      "Epoch [36/50], Step [130/6], Loss: 0.7260\n",
      "Epoch [36/50], Step [140/6], Loss: 1.3501\n",
      "Epoch [36/50], Step [150/6], Loss: 0.5249\n",
      "Epoch [36/50], Step [160/6], Loss: 0.7111\n",
      "Epoch [36/50], Step [170/6], Loss: 0.5244\n",
      "Epoch [36/50], Step [180/6], Loss: 0.2712\n",
      "Epoch [36/50], Step [190/6], Loss: 0.6697\n",
      "Epoch [36/50], Step [200/6], Loss: 0.5114\n",
      "Epoch [36/50], Step [210/6], Loss: 0.6514\n",
      "Epoch [36/50], Step [220/6], Loss: 0.6806\n",
      "Epoch [36/50], Step [230/6], Loss: 0.4580\n",
      "Epoch [36/50], Step [240/6], Loss: 0.7688\n",
      "Epoch [36/50], Step [250/6], Loss: 0.4796\n",
      "Epoch [36/50], Step [260/6], Loss: 0.9004\n",
      "Epoch [36/50], Step [270/6], Loss: 0.4388\n",
      "Epoch [36/50], Step [280/6], Loss: 0.3458\n",
      "Epoch [36/50], Step [290/6], Loss: 0.5428\n",
      "Epoch [36/50], Step [300/6], Loss: 0.4550\n",
      "Epoch [36/50], Step [310/6], Loss: 0.6903\n",
      "Epoch [36/50], Step [320/6], Loss: 0.7773\n",
      "Epoch [36/50], Step [330/6], Loss: 0.3829\n",
      "Epoch [36/50], Step [340/6], Loss: 0.4315\n",
      "Epoch [36/50], Step [350/6], Loss: 0.5925\n",
      "Epoch [36/50], Step [360/6], Loss: 0.7277\n",
      "Epoch [36/50], Step [370/6], Loss: 0.6390\n",
      "Epoch [37/50], Step [10/6], Loss: 0.5516\n",
      "Epoch [37/50], Step [20/6], Loss: 0.4628\n",
      "Epoch [37/50], Step [30/6], Loss: 0.4378\n",
      "Epoch [37/50], Step [40/6], Loss: 0.5747\n",
      "Epoch [37/50], Step [50/6], Loss: 0.4443\n",
      "Epoch [37/50], Step [60/6], Loss: 0.6491\n",
      "Epoch [37/50], Step [70/6], Loss: 0.6007\n",
      "Epoch [37/50], Step [80/6], Loss: 0.8383\n",
      "Epoch [37/50], Step [90/6], Loss: 0.3904\n",
      "Epoch [37/50], Step [100/6], Loss: 1.0163\n",
      "Epoch [37/50], Step [110/6], Loss: 0.5014\n",
      "Epoch [37/50], Step [120/6], Loss: 0.2439\n",
      "Epoch [37/50], Step [130/6], Loss: 0.7145\n",
      "Epoch [37/50], Step [140/6], Loss: 1.3652\n",
      "Epoch [37/50], Step [150/6], Loss: 0.5165\n",
      "Epoch [37/50], Step [160/6], Loss: 0.7051\n",
      "Epoch [37/50], Step [170/6], Loss: 0.5246\n",
      "Epoch [37/50], Step [180/6], Loss: 0.2631\n",
      "Epoch [37/50], Step [190/6], Loss: 0.6666\n",
      "Epoch [37/50], Step [200/6], Loss: 0.5119\n",
      "Epoch [37/50], Step [210/6], Loss: 0.6411\n",
      "Epoch [37/50], Step [220/6], Loss: 0.6695\n",
      "Epoch [37/50], Step [230/6], Loss: 0.4494\n",
      "Epoch [37/50], Step [240/6], Loss: 0.7754\n",
      "Epoch [37/50], Step [250/6], Loss: 0.4747\n",
      "Epoch [37/50], Step [260/6], Loss: 0.9108\n",
      "Epoch [37/50], Step [270/6], Loss: 0.4366\n",
      "Epoch [37/50], Step [280/6], Loss: 0.3444\n",
      "Epoch [37/50], Step [290/6], Loss: 0.5357\n",
      "Epoch [37/50], Step [300/6], Loss: 0.4486\n",
      "Epoch [37/50], Step [310/6], Loss: 0.6975\n",
      "Epoch [37/50], Step [320/6], Loss: 0.7646\n",
      "Epoch [37/50], Step [330/6], Loss: 0.3823\n",
      "Epoch [37/50], Step [340/6], Loss: 0.4175\n",
      "Epoch [37/50], Step [350/6], Loss: 0.5793\n",
      "Epoch [37/50], Step [360/6], Loss: 0.7189\n",
      "Epoch [37/50], Step [370/6], Loss: 0.6413\n",
      "Epoch [38/50], Step [10/6], Loss: 0.5455\n",
      "Epoch [38/50], Step [20/6], Loss: 0.4565\n",
      "Epoch [38/50], Step [30/6], Loss: 0.4239\n",
      "Epoch [38/50], Step [40/6], Loss: 0.5828\n",
      "Epoch [38/50], Step [50/6], Loss: 0.4482\n",
      "Epoch [38/50], Step [60/6], Loss: 0.6554\n",
      "Epoch [38/50], Step [70/6], Loss: 0.5802\n",
      "Epoch [38/50], Step [80/6], Loss: 0.8360\n",
      "Epoch [38/50], Step [90/6], Loss: 0.3856\n",
      "Epoch [38/50], Step [100/6], Loss: 0.9939\n",
      "Epoch [38/50], Step [110/6], Loss: 0.5109\n",
      "Epoch [38/50], Step [120/6], Loss: 0.2382\n",
      "Epoch [38/50], Step [130/6], Loss: 0.7027\n",
      "Epoch [38/50], Step [140/6], Loss: 1.3581\n",
      "Epoch [38/50], Step [150/6], Loss: 0.5137\n",
      "Epoch [38/50], Step [160/6], Loss: 0.6888\n",
      "Epoch [38/50], Step [170/6], Loss: 0.5270\n",
      "Epoch [38/50], Step [180/6], Loss: 0.2543\n",
      "Epoch [38/50], Step [190/6], Loss: 0.6580\n",
      "Epoch [38/50], Step [200/6], Loss: 0.5100\n",
      "Epoch [38/50], Step [210/6], Loss: 0.6313\n",
      "Epoch [38/50], Step [220/6], Loss: 0.6613\n",
      "Epoch [38/50], Step [230/6], Loss: 0.4449\n",
      "Epoch [38/50], Step [240/6], Loss: 0.7793\n",
      "Epoch [38/50], Step [250/6], Loss: 0.4689\n",
      "Epoch [38/50], Step [260/6], Loss: 0.9205\n",
      "Epoch [38/50], Step [270/6], Loss: 0.4314\n",
      "Epoch [38/50], Step [280/6], Loss: 0.3401\n",
      "Epoch [38/50], Step [290/6], Loss: 0.5275\n",
      "Epoch [38/50], Step [300/6], Loss: 0.4495\n",
      "Epoch [38/50], Step [310/6], Loss: 0.6923\n",
      "Epoch [38/50], Step [320/6], Loss: 0.7802\n",
      "Epoch [38/50], Step [330/6], Loss: 0.3802\n",
      "Epoch [38/50], Step [340/6], Loss: 0.4100\n",
      "Epoch [38/50], Step [350/6], Loss: 0.5957\n",
      "Epoch [38/50], Step [360/6], Loss: 0.7385\n",
      "Epoch [38/50], Step [370/6], Loss: 0.6455\n",
      "Epoch [39/50], Step [10/6], Loss: 0.5300\n",
      "Epoch [39/50], Step [20/6], Loss: 0.4545\n",
      "Epoch [39/50], Step [30/6], Loss: 0.4268\n",
      "Epoch [39/50], Step [40/6], Loss: 0.6025\n",
      "Epoch [39/50], Step [50/6], Loss: 0.4357\n",
      "Epoch [39/50], Step [60/6], Loss: 0.6591\n",
      "Epoch [39/50], Step [70/6], Loss: 0.5804\n",
      "Epoch [39/50], Step [80/6], Loss: 0.8378\n",
      "Epoch [39/50], Step [90/6], Loss: 0.3823\n",
      "Epoch [39/50], Step [100/6], Loss: 1.0324\n",
      "Epoch [39/50], Step [110/6], Loss: 0.5008\n",
      "Epoch [39/50], Step [120/6], Loss: 0.2383\n",
      "Epoch [39/50], Step [130/6], Loss: 0.6928\n",
      "Epoch [39/50], Step [140/6], Loss: 1.3481\n",
      "Epoch [39/50], Step [150/6], Loss: 0.5068\n",
      "Epoch [39/50], Step [160/6], Loss: 0.6789\n",
      "Epoch [39/50], Step [170/6], Loss: 0.5260\n",
      "Epoch [39/50], Step [180/6], Loss: 0.2493\n",
      "Epoch [39/50], Step [190/6], Loss: 0.6543\n",
      "Epoch [39/50], Step [200/6], Loss: 0.5127\n",
      "Epoch [39/50], Step [210/6], Loss: 0.6236\n",
      "Epoch [39/50], Step [220/6], Loss: 0.6528\n",
      "Epoch [39/50], Step [230/6], Loss: 0.4382\n",
      "Epoch [39/50], Step [240/6], Loss: 0.7838\n",
      "Epoch [39/50], Step [250/6], Loss: 0.4660\n",
      "Epoch [39/50], Step [260/6], Loss: 0.9282\n",
      "Epoch [39/50], Step [270/6], Loss: 0.4291\n",
      "Epoch [39/50], Step [280/6], Loss: 0.3669\n",
      "Epoch [39/50], Step [290/6], Loss: 0.5211\n",
      "Epoch [39/50], Step [300/6], Loss: 0.4412\n",
      "Epoch [39/50], Step [310/6], Loss: 0.6939\n",
      "Epoch [39/50], Step [320/6], Loss: 0.7729\n",
      "Epoch [39/50], Step [330/6], Loss: 0.3878\n",
      "Epoch [39/50], Step [340/6], Loss: 0.3954\n",
      "Epoch [39/50], Step [350/6], Loss: 0.5893\n",
      "Epoch [39/50], Step [360/6], Loss: 0.7371\n",
      "Epoch [39/50], Step [370/6], Loss: 0.6486\n",
      "Epoch [40/50], Step [10/6], Loss: 0.5262\n",
      "Epoch [40/50], Step [20/6], Loss: 0.4505\n",
      "Epoch [40/50], Step [30/6], Loss: 0.4124\n",
      "Epoch [40/50], Step [40/6], Loss: 0.5964\n",
      "Epoch [40/50], Step [50/6], Loss: 0.4389\n",
      "Epoch [40/50], Step [60/6], Loss: 0.6613\n",
      "Epoch [40/50], Step [70/6], Loss: 0.5682\n",
      "Epoch [40/50], Step [80/6], Loss: 0.8446\n",
      "Epoch [40/50], Step [90/6], Loss: 0.3724\n",
      "Epoch [40/50], Step [100/6], Loss: 1.0120\n",
      "Epoch [40/50], Step [110/6], Loss: 0.5062\n",
      "Epoch [40/50], Step [120/6], Loss: 0.2299\n",
      "Epoch [40/50], Step [130/6], Loss: 0.6845\n",
      "Epoch [40/50], Step [140/6], Loss: 1.3443\n",
      "Epoch [40/50], Step [150/6], Loss: 0.4964\n",
      "Epoch [40/50], Step [160/6], Loss: 0.6695\n",
      "Epoch [40/50], Step [170/6], Loss: 0.5309\n",
      "Epoch [40/50], Step [180/6], Loss: 0.2447\n",
      "Epoch [40/50], Step [190/6], Loss: 0.6440\n",
      "Epoch [40/50], Step [200/6], Loss: 0.5167\n",
      "Epoch [40/50], Step [210/6], Loss: 0.6155\n",
      "Epoch [40/50], Step [220/6], Loss: 0.6531\n",
      "Epoch [40/50], Step [230/6], Loss: 0.4318\n",
      "Epoch [40/50], Step [240/6], Loss: 0.7832\n",
      "Epoch [40/50], Step [250/6], Loss: 0.4541\n",
      "Epoch [40/50], Step [260/6], Loss: 0.9431\n",
      "Epoch [40/50], Step [270/6], Loss: 0.4274\n",
      "Epoch [40/50], Step [280/6], Loss: 0.3661\n",
      "Epoch [40/50], Step [290/6], Loss: 0.5130\n",
      "Epoch [40/50], Step [300/6], Loss: 0.4395\n",
      "Epoch [40/50], Step [310/6], Loss: 0.6869\n",
      "Epoch [40/50], Step [320/6], Loss: 0.7857\n",
      "Epoch [40/50], Step [330/6], Loss: 0.3930\n",
      "Epoch [40/50], Step [340/6], Loss: 0.3871\n",
      "Epoch [40/50], Step [350/6], Loss: 0.6016\n",
      "Epoch [40/50], Step [360/6], Loss: 0.7564\n",
      "Epoch [40/50], Step [370/6], Loss: 0.6507\n",
      "Epoch [41/50], Step [10/6], Loss: 0.5158\n",
      "Epoch [41/50], Step [20/6], Loss: 0.4498\n",
      "Epoch [41/50], Step [30/6], Loss: 0.4163\n",
      "Epoch [41/50], Step [40/6], Loss: 0.5794\n",
      "Epoch [41/50], Step [50/6], Loss: 0.4311\n",
      "Epoch [41/50], Step [60/6], Loss: 0.6664\n",
      "Epoch [41/50], Step [70/6], Loss: 0.5554\n",
      "Epoch [41/50], Step [80/6], Loss: 0.8639\n",
      "Epoch [41/50], Step [90/6], Loss: 0.3584\n",
      "Epoch [41/50], Step [100/6], Loss: 1.0121\n",
      "Epoch [41/50], Step [110/6], Loss: 0.5114\n",
      "Epoch [41/50], Step [120/6], Loss: 0.2199\n",
      "Epoch [41/50], Step [130/6], Loss: 0.6708\n",
      "Epoch [41/50], Step [140/6], Loss: 1.3670\n",
      "Epoch [41/50], Step [150/6], Loss: 0.4901\n",
      "Epoch [41/50], Step [160/6], Loss: 0.6603\n",
      "Epoch [41/50], Step [170/6], Loss: 0.5301\n",
      "Epoch [41/50], Step [180/6], Loss: 0.2367\n",
      "Epoch [41/50], Step [190/6], Loss: 0.6428\n",
      "Epoch [41/50], Step [200/6], Loss: 0.5154\n",
      "Epoch [41/50], Step [210/6], Loss: 0.6110\n",
      "Epoch [41/50], Step [220/6], Loss: 0.6390\n",
      "Epoch [41/50], Step [230/6], Loss: 0.4284\n",
      "Epoch [41/50], Step [240/6], Loss: 0.7876\n",
      "Epoch [41/50], Step [250/6], Loss: 0.4537\n",
      "Epoch [41/50], Step [260/6], Loss: 0.9466\n",
      "Epoch [41/50], Step [270/6], Loss: 0.4260\n",
      "Epoch [41/50], Step [280/6], Loss: 0.3606\n",
      "Epoch [41/50], Step [290/6], Loss: 0.5065\n",
      "Epoch [41/50], Step [300/6], Loss: 0.4393\n",
      "Epoch [41/50], Step [310/6], Loss: 0.6847\n",
      "Epoch [41/50], Step [320/6], Loss: 0.7946\n",
      "Epoch [41/50], Step [330/6], Loss: 0.3890\n",
      "Epoch [41/50], Step [340/6], Loss: 0.3788\n",
      "Epoch [41/50], Step [350/6], Loss: 0.6171\n",
      "Epoch [41/50], Step [360/6], Loss: 0.7648\n",
      "Epoch [41/50], Step [370/6], Loss: 0.6503\n",
      "Epoch [42/50], Step [10/6], Loss: 0.5021\n",
      "Epoch [42/50], Step [20/6], Loss: 0.4498\n",
      "Epoch [42/50], Step [30/6], Loss: 0.4076\n",
      "Epoch [42/50], Step [40/6], Loss: 0.6470\n",
      "Epoch [42/50], Step [50/6], Loss: 0.4165\n",
      "Epoch [42/50], Step [60/6], Loss: 0.6678\n",
      "Epoch [42/50], Step [70/6], Loss: 0.5620\n",
      "Epoch [42/50], Step [80/6], Loss: 0.8239\n",
      "Epoch [42/50], Step [90/6], Loss: 0.3664\n",
      "Epoch [42/50], Step [100/6], Loss: 1.0314\n",
      "Epoch [42/50], Step [110/6], Loss: 0.5074\n",
      "Epoch [42/50], Step [120/6], Loss: 0.2219\n",
      "Epoch [42/50], Step [130/6], Loss: 0.6618\n",
      "Epoch [42/50], Step [140/6], Loss: 1.3678\n",
      "Epoch [42/50], Step [150/6], Loss: 0.4847\n",
      "Epoch [42/50], Step [160/6], Loss: 0.6560\n",
      "Epoch [42/50], Step [170/6], Loss: 0.5242\n",
      "Epoch [42/50], Step [180/6], Loss: 0.2305\n",
      "Epoch [42/50], Step [190/6], Loss: 0.6346\n",
      "Epoch [42/50], Step [200/6], Loss: 0.5150\n",
      "Epoch [42/50], Step [210/6], Loss: 0.6029\n",
      "Epoch [42/50], Step [220/6], Loss: 0.6359\n",
      "Epoch [42/50], Step [230/6], Loss: 0.4264\n",
      "Epoch [42/50], Step [240/6], Loss: 0.7951\n",
      "Epoch [42/50], Step [250/6], Loss: 0.4487\n",
      "Epoch [42/50], Step [260/6], Loss: 0.9459\n",
      "Epoch [42/50], Step [270/6], Loss: 0.4419\n",
      "Epoch [42/50], Step [280/6], Loss: 0.3660\n",
      "Epoch [42/50], Step [290/6], Loss: 0.5011\n",
      "Epoch [42/50], Step [300/6], Loss: 0.4366\n",
      "Epoch [42/50], Step [310/6], Loss: 0.6816\n",
      "Epoch [42/50], Step [320/6], Loss: 0.7922\n",
      "Epoch [42/50], Step [330/6], Loss: 0.3901\n",
      "Epoch [42/50], Step [340/6], Loss: 0.3673\n",
      "Epoch [42/50], Step [350/6], Loss: 0.6257\n",
      "Epoch [42/50], Step [360/6], Loss: 0.7772\n",
      "Epoch [42/50], Step [370/6], Loss: 0.6500\n",
      "Epoch [43/50], Step [10/6], Loss: 0.4952\n",
      "Epoch [43/50], Step [20/6], Loss: 0.4530\n",
      "Epoch [43/50], Step [30/6], Loss: 0.4081\n",
      "Epoch [43/50], Step [40/6], Loss: 0.6129\n",
      "Epoch [43/50], Step [50/6], Loss: 0.4120\n",
      "Epoch [43/50], Step [60/6], Loss: 0.6670\n",
      "Epoch [43/50], Step [70/6], Loss: 0.5689\n",
      "Epoch [43/50], Step [80/6], Loss: 0.8559\n",
      "Epoch [43/50], Step [90/6], Loss: 0.3531\n",
      "Epoch [43/50], Step [100/6], Loss: 1.0346\n",
      "Epoch [43/50], Step [110/6], Loss: 0.5033\n",
      "Epoch [43/50], Step [120/6], Loss: 0.2134\n",
      "Epoch [43/50], Step [130/6], Loss: 0.6519\n",
      "Epoch [43/50], Step [140/6], Loss: 1.3482\n",
      "Epoch [43/50], Step [150/6], Loss: 0.4778\n",
      "Epoch [43/50], Step [160/6], Loss: 0.6420\n",
      "Epoch [43/50], Step [170/6], Loss: 0.5329\n",
      "Epoch [43/50], Step [180/6], Loss: 0.2228\n",
      "Epoch [43/50], Step [190/6], Loss: 0.6198\n",
      "Epoch [43/50], Step [200/6], Loss: 0.5223\n",
      "Epoch [43/50], Step [210/6], Loss: 0.5993\n",
      "Epoch [43/50], Step [220/6], Loss: 0.6330\n",
      "Epoch [43/50], Step [230/6], Loss: 0.4265\n",
      "Epoch [43/50], Step [240/6], Loss: 0.8071\n",
      "Epoch [43/50], Step [250/6], Loss: 0.4433\n",
      "Epoch [43/50], Step [260/6], Loss: 0.9481\n",
      "Epoch [43/50], Step [270/6], Loss: 0.4401\n",
      "Epoch [43/50], Step [280/6], Loss: 0.3566\n",
      "Epoch [43/50], Step [290/6], Loss: 0.4958\n",
      "Epoch [43/50], Step [300/6], Loss: 0.4363\n",
      "Epoch [43/50], Step [310/6], Loss: 0.6761\n",
      "Epoch [43/50], Step [320/6], Loss: 0.7830\n",
      "Epoch [43/50], Step [330/6], Loss: 0.3878\n",
      "Epoch [43/50], Step [340/6], Loss: 0.3586\n",
      "Epoch [43/50], Step [350/6], Loss: 0.6369\n",
      "Epoch [43/50], Step [360/6], Loss: 0.7883\n",
      "Epoch [43/50], Step [370/6], Loss: 0.6544\n",
      "Epoch [44/50], Step [10/6], Loss: 0.4873\n",
      "Epoch [44/50], Step [20/6], Loss: 0.4535\n",
      "Epoch [44/50], Step [30/6], Loss: 0.4009\n",
      "Epoch [44/50], Step [40/6], Loss: 0.6062\n",
      "Epoch [44/50], Step [50/6], Loss: 0.4076\n",
      "Epoch [44/50], Step [60/6], Loss: 0.6754\n",
      "Epoch [44/50], Step [70/6], Loss: 0.5584\n",
      "Epoch [44/50], Step [80/6], Loss: 0.8626\n",
      "Epoch [44/50], Step [90/6], Loss: 0.3436\n",
      "Epoch [44/50], Step [100/6], Loss: 1.0310\n",
      "Epoch [44/50], Step [110/6], Loss: 0.4945\n",
      "Epoch [44/50], Step [120/6], Loss: 0.2107\n",
      "Epoch [44/50], Step [130/6], Loss: 0.6390\n",
      "Epoch [44/50], Step [140/6], Loss: 1.3694\n",
      "Epoch [44/50], Step [150/6], Loss: 0.4720\n",
      "Epoch [44/50], Step [160/6], Loss: 0.6316\n",
      "Epoch [44/50], Step [170/6], Loss: 0.5292\n",
      "Epoch [44/50], Step [180/6], Loss: 0.2167\n",
      "Epoch [44/50], Step [190/6], Loss: 0.6101\n",
      "Epoch [44/50], Step [200/6], Loss: 0.5172\n",
      "Epoch [44/50], Step [210/6], Loss: 0.5925\n",
      "Epoch [44/50], Step [220/6], Loss: 0.6359\n",
      "Epoch [44/50], Step [230/6], Loss: 0.4210\n",
      "Epoch [44/50], Step [240/6], Loss: 0.8051\n",
      "Epoch [44/50], Step [250/6], Loss: 0.4374\n",
      "Epoch [44/50], Step [260/6], Loss: 0.9522\n",
      "Epoch [44/50], Step [270/6], Loss: 0.4364\n",
      "Epoch [44/50], Step [280/6], Loss: 0.3637\n",
      "Epoch [44/50], Step [290/6], Loss: 0.4895\n",
      "Epoch [44/50], Step [300/6], Loss: 0.4348\n",
      "Epoch [44/50], Step [310/6], Loss: 0.6774\n",
      "Epoch [44/50], Step [320/6], Loss: 0.7850\n",
      "Epoch [44/50], Step [330/6], Loss: 0.3884\n",
      "Epoch [44/50], Step [340/6], Loss: 0.3498\n",
      "Epoch [44/50], Step [350/6], Loss: 0.6414\n",
      "Epoch [44/50], Step [360/6], Loss: 0.7840\n",
      "Epoch [44/50], Step [370/6], Loss: 0.6550\n",
      "Epoch [45/50], Step [10/6], Loss: 0.4792\n",
      "Epoch [45/50], Step [20/6], Loss: 0.4452\n",
      "Epoch [45/50], Step [30/6], Loss: 0.3914\n",
      "Epoch [45/50], Step [40/6], Loss: 0.6193\n",
      "Epoch [45/50], Step [50/6], Loss: 0.3968\n",
      "Epoch [45/50], Step [60/6], Loss: 0.6742\n",
      "Epoch [45/50], Step [70/6], Loss: 0.5543\n",
      "Epoch [45/50], Step [80/6], Loss: 0.8718\n",
      "Epoch [45/50], Step [90/6], Loss: 0.3348\n",
      "Epoch [45/50], Step [100/6], Loss: 1.0314\n",
      "Epoch [45/50], Step [110/6], Loss: 0.5055\n",
      "Epoch [45/50], Step [120/6], Loss: 0.2020\n",
      "Epoch [45/50], Step [130/6], Loss: 0.6315\n",
      "Epoch [45/50], Step [140/6], Loss: 1.3784\n",
      "Epoch [45/50], Step [150/6], Loss: 0.4628\n",
      "Epoch [45/50], Step [160/6], Loss: 0.6286\n",
      "Epoch [45/50], Step [170/6], Loss: 0.5302\n",
      "Epoch [45/50], Step [180/6], Loss: 0.2085\n",
      "Epoch [45/50], Step [190/6], Loss: 0.6020\n",
      "Epoch [45/50], Step [200/6], Loss: 0.5179\n",
      "Epoch [45/50], Step [210/6], Loss: 0.5926\n",
      "Epoch [45/50], Step [220/6], Loss: 0.6196\n",
      "Epoch [45/50], Step [230/6], Loss: 0.4207\n",
      "Epoch [45/50], Step [240/6], Loss: 0.8131\n",
      "Epoch [45/50], Step [250/6], Loss: 0.4382\n",
      "Epoch [45/50], Step [260/6], Loss: 0.9576\n",
      "Epoch [45/50], Step [270/6], Loss: 0.4442\n",
      "Epoch [45/50], Step [280/6], Loss: 0.3652\n",
      "Epoch [45/50], Step [290/6], Loss: 0.4828\n",
      "Epoch [45/50], Step [300/6], Loss: 0.4322\n",
      "Epoch [45/50], Step [310/6], Loss: 0.6741\n",
      "Epoch [45/50], Step [320/6], Loss: 0.7847\n",
      "Epoch [45/50], Step [330/6], Loss: 0.3845\n",
      "Epoch [45/50], Step [340/6], Loss: 0.3423\n",
      "Epoch [45/50], Step [350/6], Loss: 0.6478\n",
      "Epoch [45/50], Step [360/6], Loss: 0.7830\n",
      "Epoch [45/50], Step [370/6], Loss: 0.6477\n",
      "Epoch [46/50], Step [10/6], Loss: 0.4687\n",
      "Epoch [46/50], Step [20/6], Loss: 0.4511\n",
      "Epoch [46/50], Step [30/6], Loss: 0.3900\n",
      "Epoch [46/50], Step [40/6], Loss: 0.6635\n",
      "Epoch [46/50], Step [50/6], Loss: 0.3884\n",
      "Epoch [46/50], Step [60/6], Loss: 0.6690\n",
      "Epoch [46/50], Step [70/6], Loss: 0.5593\n",
      "Epoch [46/50], Step [80/6], Loss: 0.8161\n",
      "Epoch [46/50], Step [90/6], Loss: 0.3532\n",
      "Epoch [46/50], Step [100/6], Loss: 1.0410\n",
      "Epoch [46/50], Step [110/6], Loss: 0.4995\n",
      "Epoch [46/50], Step [120/6], Loss: 0.2012\n",
      "Epoch [46/50], Step [130/6], Loss: 0.6328\n",
      "Epoch [46/50], Step [140/6], Loss: 1.3653\n",
      "Epoch [46/50], Step [150/6], Loss: 0.4467\n",
      "Epoch [46/50], Step [160/6], Loss: 0.6241\n",
      "Epoch [46/50], Step [170/6], Loss: 0.5321\n",
      "Epoch [46/50], Step [180/6], Loss: 0.2033\n",
      "Epoch [46/50], Step [190/6], Loss: 0.5976\n",
      "Epoch [46/50], Step [200/6], Loss: 0.5211\n",
      "Epoch [46/50], Step [210/6], Loss: 0.5890\n",
      "Epoch [46/50], Step [220/6], Loss: 0.6226\n",
      "Epoch [46/50], Step [230/6], Loss: 0.4180\n",
      "Epoch [46/50], Step [240/6], Loss: 0.8215\n",
      "Epoch [46/50], Step [250/6], Loss: 0.4340\n",
      "Epoch [46/50], Step [260/6], Loss: 0.9592\n",
      "Epoch [46/50], Step [270/6], Loss: 0.4377\n",
      "Epoch [46/50], Step [280/6], Loss: 0.3606\n",
      "Epoch [46/50], Step [290/6], Loss: 0.4764\n",
      "Epoch [46/50], Step [300/6], Loss: 0.4232\n",
      "Epoch [46/50], Step [310/6], Loss: 0.6757\n",
      "Epoch [46/50], Step [320/6], Loss: 0.7884\n",
      "Epoch [46/50], Step [330/6], Loss: 0.3887\n",
      "Epoch [46/50], Step [340/6], Loss: 0.3367\n",
      "Epoch [46/50], Step [350/6], Loss: 0.6518\n",
      "Epoch [46/50], Step [360/6], Loss: 0.7895\n",
      "Epoch [46/50], Step [370/6], Loss: 0.6506\n",
      "Epoch [47/50], Step [10/6], Loss: 0.4606\n",
      "Epoch [47/50], Step [20/6], Loss: 0.4499\n",
      "Epoch [47/50], Step [30/6], Loss: 0.3832\n",
      "Epoch [47/50], Step [40/6], Loss: 0.6508\n",
      "Epoch [47/50], Step [50/6], Loss: 0.3914\n",
      "Epoch [47/50], Step [60/6], Loss: 0.6725\n",
      "Epoch [47/50], Step [70/6], Loss: 0.5524\n",
      "Epoch [47/50], Step [80/6], Loss: 0.8126\n",
      "Epoch [47/50], Step [90/6], Loss: 0.3410\n",
      "Epoch [47/50], Step [100/6], Loss: 1.0383\n",
      "Epoch [47/50], Step [110/6], Loss: 0.4953\n",
      "Epoch [47/50], Step [120/6], Loss: 0.1940\n",
      "Epoch [47/50], Step [130/6], Loss: 0.6267\n",
      "Epoch [47/50], Step [140/6], Loss: 1.3844\n",
      "Epoch [47/50], Step [150/6], Loss: 0.4443\n",
      "Epoch [47/50], Step [160/6], Loss: 0.6123\n",
      "Epoch [47/50], Step [170/6], Loss: 0.5307\n",
      "Epoch [47/50], Step [180/6], Loss: 0.1968\n",
      "Epoch [47/50], Step [190/6], Loss: 0.5906\n",
      "Epoch [47/50], Step [200/6], Loss: 0.5209\n",
      "Epoch [47/50], Step [210/6], Loss: 0.5828\n",
      "Epoch [47/50], Step [220/6], Loss: 0.6260\n",
      "Epoch [47/50], Step [230/6], Loss: 0.4104\n",
      "Epoch [47/50], Step [240/6], Loss: 0.8199\n",
      "Epoch [47/50], Step [250/6], Loss: 0.4294\n",
      "Epoch [47/50], Step [260/6], Loss: 0.9658\n",
      "Epoch [47/50], Step [270/6], Loss: 0.4359\n",
      "Epoch [47/50], Step [280/6], Loss: 0.3639\n",
      "Epoch [47/50], Step [290/6], Loss: 0.4695\n",
      "Epoch [47/50], Step [300/6], Loss: 0.4243\n",
      "Epoch [47/50], Step [310/6], Loss: 0.6765\n",
      "Epoch [47/50], Step [320/6], Loss: 0.7873\n",
      "Epoch [47/50], Step [330/6], Loss: 0.3877\n",
      "Epoch [47/50], Step [340/6], Loss: 0.3300\n",
      "Epoch [47/50], Step [350/6], Loss: 0.6576\n",
      "Epoch [47/50], Step [360/6], Loss: 0.7957\n",
      "Epoch [47/50], Step [370/6], Loss: 0.6508\n",
      "Epoch [48/50], Step [10/6], Loss: 0.4543\n",
      "Epoch [48/50], Step [20/6], Loss: 0.4506\n",
      "Epoch [48/50], Step [30/6], Loss: 0.3809\n",
      "Epoch [48/50], Step [40/6], Loss: 0.5738\n",
      "Epoch [48/50], Step [50/6], Loss: 0.3876\n",
      "Epoch [48/50], Step [60/6], Loss: 0.6782\n",
      "Epoch [48/50], Step [70/6], Loss: 0.5451\n",
      "Epoch [48/50], Step [80/6], Loss: 0.9095\n",
      "Epoch [48/50], Step [90/6], Loss: 0.3119\n",
      "Epoch [48/50], Step [100/6], Loss: 1.0314\n",
      "Epoch [48/50], Step [110/6], Loss: 0.5029\n",
      "Epoch [48/50], Step [120/6], Loss: 0.1841\n",
      "Epoch [48/50], Step [130/6], Loss: 0.6125\n",
      "Epoch [48/50], Step [140/6], Loss: 1.3827\n",
      "Epoch [48/50], Step [150/6], Loss: 0.4459\n",
      "Epoch [48/50], Step [160/6], Loss: 0.6050\n",
      "Epoch [48/50], Step [170/6], Loss: 0.5347\n",
      "Epoch [48/50], Step [180/6], Loss: 0.1898\n",
      "Epoch [48/50], Step [190/6], Loss: 0.5920\n",
      "Epoch [48/50], Step [200/6], Loss: 0.5242\n",
      "Epoch [48/50], Step [210/6], Loss: 0.5738\n",
      "Epoch [48/50], Step [220/6], Loss: 0.6154\n",
      "Epoch [48/50], Step [230/6], Loss: 0.4103\n",
      "Epoch [48/50], Step [240/6], Loss: 0.8269\n",
      "Epoch [48/50], Step [250/6], Loss: 0.4258\n",
      "Epoch [48/50], Step [260/6], Loss: 0.9686\n",
      "Epoch [48/50], Step [270/6], Loss: 0.4342\n",
      "Epoch [48/50], Step [280/6], Loss: 0.3636\n",
      "Epoch [48/50], Step [290/6], Loss: 0.4635\n",
      "Epoch [48/50], Step [300/6], Loss: 0.4271\n",
      "Epoch [48/50], Step [310/6], Loss: 0.6805\n",
      "Epoch [48/50], Step [320/6], Loss: 0.7937\n",
      "Epoch [48/50], Step [330/6], Loss: 0.3810\n",
      "Epoch [48/50], Step [340/6], Loss: 0.3231\n",
      "Epoch [48/50], Step [350/6], Loss: 0.6619\n",
      "Epoch [48/50], Step [360/6], Loss: 0.7920\n",
      "Epoch [48/50], Step [370/6], Loss: 0.6512\n",
      "Epoch [49/50], Step [10/6], Loss: 0.4488\n",
      "Epoch [49/50], Step [20/6], Loss: 0.4431\n",
      "Epoch [49/50], Step [30/6], Loss: 0.3698\n",
      "Epoch [49/50], Step [40/6], Loss: 0.6265\n",
      "Epoch [49/50], Step [50/6], Loss: 0.3841\n",
      "Epoch [49/50], Step [60/6], Loss: 0.6764\n",
      "Epoch [49/50], Step [70/6], Loss: 0.5407\n",
      "Epoch [49/50], Step [80/6], Loss: 0.8798\n",
      "Epoch [49/50], Step [90/6], Loss: 0.3113\n",
      "Epoch [49/50], Step [100/6], Loss: 1.0324\n",
      "Epoch [49/50], Step [110/6], Loss: 0.5022\n",
      "Epoch [49/50], Step [120/6], Loss: 0.1788\n",
      "Epoch [49/50], Step [130/6], Loss: 0.6097\n",
      "Epoch [49/50], Step [140/6], Loss: 1.3502\n",
      "Epoch [49/50], Step [150/6], Loss: 0.4367\n",
      "Epoch [49/50], Step [160/6], Loss: 0.6035\n",
      "Epoch [49/50], Step [170/6], Loss: 0.5311\n",
      "Epoch [49/50], Step [180/6], Loss: 0.1839\n",
      "Epoch [49/50], Step [190/6], Loss: 0.5835\n",
      "Epoch [49/50], Step [200/6], Loss: 0.5211\n",
      "Epoch [49/50], Step [210/6], Loss: 0.5772\n",
      "Epoch [49/50], Step [220/6], Loss: 0.6182\n",
      "Epoch [49/50], Step [230/6], Loss: 0.4031\n",
      "Epoch [49/50], Step [240/6], Loss: 0.8286\n",
      "Epoch [49/50], Step [250/6], Loss: 0.4246\n",
      "Epoch [49/50], Step [260/6], Loss: 0.9694\n",
      "Epoch [49/50], Step [270/6], Loss: 0.4395\n",
      "Epoch [49/50], Step [280/6], Loss: 0.3574\n",
      "Epoch [49/50], Step [290/6], Loss: 0.4583\n",
      "Epoch [49/50], Step [300/6], Loss: 0.4250\n",
      "Epoch [49/50], Step [310/6], Loss: 0.6799\n",
      "Epoch [49/50], Step [320/6], Loss: 0.7820\n",
      "Epoch [49/50], Step [330/6], Loss: 0.3845\n",
      "Epoch [49/50], Step [340/6], Loss: 0.3161\n",
      "Epoch [49/50], Step [350/6], Loss: 0.6683\n",
      "Epoch [49/50], Step [360/6], Loss: 0.8069\n",
      "Epoch [49/50], Step [370/6], Loss: 0.6534\n",
      "Epoch [50/50], Step [10/6], Loss: 0.4427\n",
      "Epoch [50/50], Step [20/6], Loss: 0.4493\n",
      "Epoch [50/50], Step [30/6], Loss: 0.3738\n",
      "Epoch [50/50], Step [40/6], Loss: 0.6263\n",
      "Epoch [50/50], Step [50/6], Loss: 0.3816\n",
      "Epoch [50/50], Step [60/6], Loss: 0.6797\n",
      "Epoch [50/50], Step [70/6], Loss: 0.5399\n",
      "Epoch [50/50], Step [80/6], Loss: 0.8794\n",
      "Epoch [50/50], Step [90/6], Loss: 0.3012\n",
      "Epoch [50/50], Step [100/6], Loss: 1.0292\n",
      "Epoch [50/50], Step [110/6], Loss: 0.5023\n",
      "Epoch [50/50], Step [120/6], Loss: 0.1767\n",
      "Epoch [50/50], Step [130/6], Loss: 0.6007\n",
      "Epoch [50/50], Step [140/6], Loss: 1.3830\n",
      "Epoch [50/50], Step [150/6], Loss: 0.4333\n",
      "Epoch [50/50], Step [160/6], Loss: 0.5947\n",
      "Epoch [50/50], Step [170/6], Loss: 0.5418\n",
      "Epoch [50/50], Step [180/6], Loss: 0.1802\n",
      "Epoch [50/50], Step [190/6], Loss: 0.5792\n",
      "Epoch [50/50], Step [200/6], Loss: 0.5229\n",
      "Epoch [50/50], Step [210/6], Loss: 0.5718\n",
      "Epoch [50/50], Step [220/6], Loss: 0.6154\n",
      "Epoch [50/50], Step [230/6], Loss: 0.4006\n",
      "Epoch [50/50], Step [240/6], Loss: 0.8365\n",
      "Epoch [50/50], Step [250/6], Loss: 0.4214\n",
      "Epoch [50/50], Step [260/6], Loss: 0.9774\n",
      "Epoch [50/50], Step [270/6], Loss: 0.4454\n",
      "Epoch [50/50], Step [280/6], Loss: 0.3587\n",
      "Epoch [50/50], Step [290/6], Loss: 0.4562\n",
      "Epoch [50/50], Step [300/6], Loss: 0.4217\n",
      "Epoch [50/50], Step [310/6], Loss: 0.6820\n",
      "Epoch [50/50], Step [320/6], Loss: 0.7896\n",
      "Epoch [50/50], Step [330/6], Loss: 0.3887\n",
      "Epoch [50/50], Step [340/6], Loss: 0.3100\n",
      "Epoch [50/50], Step [350/6], Loss: 0.6628\n",
      "Epoch [50/50], Step [360/6], Loss: 0.8015\n",
      "Epoch [50/50], Step [370/6], Loss: 0.6463\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (z, y) in enumerate(zip(z_matrix, data_y)):\n",
    "        z = torch.from_numpy(z).double()\n",
    "        y = torch.tensor([[y]]).double().reshape([1,1])\n",
    "        y_pred = model(z).reshape([1,1])\n",
    "\n",
    "        loss = loss_func(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(valid_loader)}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for z in z_matrix:\n",
    "        z=torch.from_numpy(z)\n",
    "        y_pred = model(z)\n",
    "        predictions.append(y_pred.item())\n",
    "    predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "predictions[predictions >= threshold] = 1\n",
    "predictions[predictions < threshold] = 0\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172413793103449"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions==np.array(data_y[valid_indices]))/len(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
